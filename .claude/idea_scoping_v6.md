# IDEA SCOPING V6 ‚Äî Architecture Solide

**AI Agent Platform for SMEs** | F√©vrier 2026 | V6 : Consolidation technique (Dust & n8n deep-dive)

---

## SUMMARY

En 2018, Guillaume Moubeche a lanc√© Lemlist dans un march√© domin√© par Outreach ($100M+ ARR), Salesloft, Mailshake, Woodpecker. Il a lanc√© un outil de prospection complet ‚Äî un CRM avec gestion de campagnes de cold email ‚Äî mais avec UNE technique que personne n'avait (les images personnalis√©es). Produit complet Day 1 + une diff√©renciation killer. Il a grandi vers la revenue acceleration platform sur 7 ans, guid√© par le feedback utilisateur. R√©sultat : $40M ARR, 50K clients, bootstrapped, 40% EBITDA.

**On fait pareil.**

On lance une plateforme d'agents AI pour PME non-tech Day 1. On r√©plique le mod√®le qui marche (Lindy : agents autonomes connect√©s aux outils business via Pipedream) avec 2 features que personne n'a : **un scan qui montre ce qui tombe entre les mailles** (deals dormants, tickets proches SLA, candidatures non trait√©es, factures overdue, t√¢ches sans assign√©, campagnes en chute) et **un style learner** (les drafts deviennent les tiens). ~93 templates couvrant sales, marketing, support, HR, ops, research/product. Le vrai wedge se d√©couvre en M1-M3 via les donn√©es d'usage.

**3 piliers :**

1. **Le produit :** Plateforme d'agents AI plug-and-play pour PME non-tech. Scan ‚Ç¨ + ~93 templates couvrant sales, marketing, support, HR, ops, productivit√©. Chaque agent se connecte √† ses outils via Pipedream et fait le job. Style Learner. Workflows customisables via React Flow.

2. **Le dogfooding total :** On g√®re notre propre bo√Æte enti√®rement avec nos agents. Prospection, follow-up, support, recrutement, meetings, content, facturation. Chaque agent qu'on utilise devient un template. Chaque r√©sultat devient du contenu. La preuve est inarguable.

3. **La communaut√© sur le craft :** Pas une communaut√© produit. Une communaut√© sur le m√©tier : "comment automatiser ta bo√Æte avec l'AI." Templates d'agents partag√©s gratuitement, r√©sultats r√©els, build in public. Les gens viennent pour apprendre le craft. Notre outil est dans chaque exemple.

---

## 1. Problem

Identique au V4. Trois layers imbriqu√©es. Sources v√©rifi√©es.

### Layer 1 ‚Äî Time hemorrhage on non-strategic tasks (known, partially addressed)

**Transversal :** **28% of French SME directors** spend minimum 2 days/week on admin (CPME survey, July 2024, n=1,612).

**Sales :** Sales reps sell only **30%** of their time (Salesforce State of Sales, 6th Edition, 2024, 5,500+ respondents). HubSpot 2024: sales reps sell only **2 hours per day**. **68%** of reps say note-taking and data input are their most time-consuming tasks (Salesroom 2024).

**Marketing :** Marketers spend **16h/semaine** sur des t√¢ches administratives et de reporting au lieu de la strat√©gie et la cr√©ation (CoSchedule 2024). **60%** du temps marketing est consacr√© √† des t√¢ches op√©rationnelles (Gartner CMO Spend Survey 2024).

**Support :** Les agents support passent **~30%** de leur temps √† chercher des informations au lieu de r√©soudre des tickets (Zendesk CX Trends 2024). Le co√ªt moyen d'un ticket humain est de **‚Ç¨15-25** alors qu'un bot bien configur√© co√ªte **<‚Ç¨1** (Intercom 2024).

**HR :** Les √©quipes RH passent **40%** de leur temps sur des t√¢ches administratives (SHRM 2024). Le co√ªt moyen d'un recrutement en France est de **‚Ç¨5,000-8,000** ‚Äî dont une large partie en process manuels (screening, scheduling, follow-ups).

**Ops :** Les chefs de projet passent **54%** de leur temps sur du "work about work" au lieu du travail strat√©gique (Asana Work Index 2024).

Partiellement adress√© par Zapier/Make/n8n. Mais ces outils automatisent des t√¢ches connues ‚Äî ils ne d√©tectent pas ce qui √©chappe.

### Layer 2 ‚Äî Data quality crisis (known, ignored)

CRM data decays at ~2.1%/month. **76%** of CRM data is inaccurate (Validity State of CRM Data Management 2025, n=602). **44%** of businesses lose >10% revenue from bad CRM data (Validity 2022, n=1,200+). Employees spend **13h/week** searching for CRM info (Validity 2025). Nobody fixes it because nobody sees it.

### Layer 3 ‚Äî Agent unreliability (unnamed, unaddressed)

**METR RCT** (Jul 2025, n=16 devs, 246 tasks): AI devs **19% slower** but believed 20% faster. **IBM CEO Study** (May 2025, n=2,000 CEOs): only **25%** of AI initiatives deliver ROI. **DORA 2024** (n=39,000+): 25% AI adoption correlates with **-7.2% delivery stability**. **Klarna** (May 2025): replaced 700 agents ‚Üí rehired. Lower quality, detected too late. **S&P Global** (2025): **42%** of enterprises scrapped AI initiatives. **France Num** (Sept 2025, n=3,043): 42% PME use AI, **only 5%** real automation.

**Le probl√®me core : les gens ne savent pas ce qui tombe entre les mailles.** Pas "je perds du temps sur des t√¢ches r√©p√©titives" (Zapier r√©pond partiellement). Mais : des deals qui glissent silencieusement dans le CRM, des tickets support qui approchent du SLA sans que personne ne le voie, des candidatures non trait√©es depuis 5 jours, des factures impay√©es qui s'accumulent, des t√¢ches overdue que personne n'a reassign√©es. Chaque m√©tier a ses trous. On le d√©couvre quand le client se plaint, le candidat accepte ailleurs, la facture passe en contentieux, ou le deal est perdu. Trop tard.

### 1.1 Who's experiencing this?

| Persona | R√¥le | Pain #1 | Pain #2 |
|---------|------|---------|---------|
| **Thomas** | Sales Director, PME 40 pers. | "Des deals dorment dans le CRM et je ne le vois pas" | "Mon CRM est une passoire mais personne ne le maintient" |
| **Sophie** | Head of Ops, PME 80 pers. | "Je ne sais pas ce qui tombe entre les mailles" | "Je n'ai pas confiance en l'AI pour agir √† ma place" |
| **Claire** | Responsable Marketing, PME 60 pers. | "Je passe plus de temps √† reporter qu'√† cr√©er" | "Je recycle du contenu manuellement sur 5 plateformes" |
| **Antoine** | DRH, PME 100 pers. | "Le screening de CV me prend 3 jours par recrutement" | "L'onboarding est un fichier Excel que personne ne suit" |
| **Julien** | Head of Support, PME 50 pers. | "Les tickets s'empilent et je ne vois pas lesquels sont critiques" | "On r√©pond toujours aux m√™mes questions" |
| **CEO PME** | Dirigeant, 10-200 pers. | "On perd des clients sans savoir pourquoi" | "Je n'ai pas de visibilit√© sur les op√©rations" |

### 1.2 Frequency & Intensity

Un employ√© moyen de PME a 20-50 engagements ouverts √† tout moment et **ne le sait pas**. Le co√ªt n'est pas le temps perdu ‚Äî c'est le revenu perdu sur les deals qui glissent, les tickets qui escaladent, les candidats qui acceptent ailleurs, les factures qui passent en contentieux, les t√¢ches qui bloquent des projets entiers. Invisible ‚Üí non-actionnable ‚Üí pertes compos√©es.

---

## 2. Solution

### 2.0 Le parall√®le Lemlist ‚Äî comment construire face aux incumbents

**Lemlist (2018) :** Outreach faisait $100M+ ARR. Salesloft √©tait massivement financ√©. Guillaume avait $1,000 et un MVP moche.

Il a lanc√© un outil de prospection complet ‚Äî un CRM dans lequel on pouvait g√©rer ses campagnes de cold email ‚Äî mais avec UN angle que personne n'avait : les images personnalis√©es dans les cold emails. Pas un prototype avec une feature. Un produit complet avec UNE technique killer.

La technique marchait parce qu'elle avait 5 propri√©t√©s :
1. **Visible en 5 secondes** ‚Äî le prospect voit son nom sur un whiteboard
2. **R√©sultat imm√©diat** ‚Äî taux de r√©ponse qui double d√®s le premier envoi
3. **Partageable** ‚Äî les gens screenshotaient les emails re√ßus
4. **R√©sout un vrai probl√®me quotidien** ‚Äî "mes cold emails sont ignor√©s"
5. **Simple √† comprendre** ‚Äî tu vois, tu comprends

Et surtout : **li√© directement au CA.** Plus de r√©ponses ‚Üí plus de meetings ‚Üí plus de deals. Le sales rep pouvait dire √† son boss : "6 meetings de plus ce mois." Inarguable.

Timeline produit Lemlist :

| Phase | Produit | Ce qui √©tait "mieux" |
|-------|---------|---------------------|
| **V1 (2018)** | CRM + cold email + images personnalis√©es | Produit complet avec UNE technique killer |
| **V2 (2019)** | Refonte onboarding (activation 10% ‚Üí 45%) | Pas de nouvelle feature ‚Äî juste utilisable |
| **V3 (2020)** | Multichannel (email + LinkedIn + calls) | Extension demand√©e par les users |
| **V4 (2021-22)** | Sales engagement platform compl√®te | Feature parity avec Outreach/Salesloft |
| **V5 (2023-25)** | Lempire (acquisitions Taplio, TweetHunter, Claap) | Revenue acceleration platform, $40M ARR |

**7 ans pour aller de "CRM avec images personnalis√©es" √† "revenue acceleration platform."** Chaque √©tape guid√©e par le feedback utilisateur.

**Notre approche :** Comme Lemlist, on lance un produit complet Day 1 ‚Äî une plateforme d'agents AI pour les non-tech (pas un prototype). ~93 templates couvrant sales, marketing, support, ops, HR, research/product. Avec 2 features que personne n'a (scan ‚Ç¨, style learner). Le vrai wedge √©mergera en M1-M3 via l'usage, comme Lemlist a d√©couvert que le multi-channel √©tait plus fort que les images.

### 2.1 Competitive Landscape ‚Äî Le gap qu'on exploite

**Concurrent #1 : le statu quo.** La majorit√© des PME n'utilisent PAS encore d'agents AI. Le Sales Director fait ses follow-ups √† la main, la RH screening les CV manuellement, le support copie-colle les m√™mes r√©ponses. On ne vend pas "mieux que Lindy" ‚Äî on vend "tes t√¢ches r√©p√©titives se font toutes seules."

**Concurrents directs ‚Äî Plateformes d'agents AI :**

| | **Dust** | **Lindy** | **Nous (V1)** |
|---|---|---|---|
| **Positionnement** | Agent platform enterprise | AI Employees no-code | Plateforme agents AI pour PME non-tech |
| **Cible** | Enterprise 3000+ | SMB/Startups, √©quipes business, ops teams | PME 10-500, non-technique |
| **Onboarding** | "Qu'est-ce que tu veux construire ?" | "Choisis parmi 1,000 templates" | **"Connecte tes outils, on te montre ce qui fuit"** |
| **Templates** | Custom par l'admin IT | 1,000+ templates | **~93 templates plug-and-play** |
| **Apprentissage** | M√©moire conversationnelle | Aucun | **Style Learner (few-shot diffs)** |
| **Frame r√©sultats** | "X agents d√©ploy√©s" | "X t√¢ches automatis√©es" | **"X alertes trait√©es / ‚Ç¨X d'impact business"** |
| **Infra** | Custom | Pipedream | **Pipedream (m√™me infra que Lindy, 2,800+ APIs)** |

**Concurrents indirects ‚Äî Outils d'automatisation :**

| | **Zapier** | **Make** | **n8n** |
|---|---|---|---|
| **Positionnement** | Automatisation par workflows | Automatisation visuelle avanc√©e | Automatisation open-source |
| **Cible** | PME, tous niveaux | PME tech-savvy | Devs, self-hosted |
| **Mod√®le** | Trigger ‚Üí Action (si X alors Y) | Scenarios visuels multi-√©tapes | Workflows programmables |
| **Limite** | Zero intelligence : ne D√âTECTE rien, n'√©crit rien, n'analyse rien. L'user doit savoir exactement ce qu'il veut automatiser. | Idem ‚Äî puissant mais r√©actif, pas proactif | Idem + n√©cessite comp√©tences techniques |
| **Ce qu'on fait diff√©remment** | Nos agents sont PROACTIFS : ils d√©tectent les deals dormants, les tickets proches SLA, les candidatures non trait√©es, les factures overdue. Zapier ne sait pas que tu as un ticket premium non assign√©. | Nos agents G√âN√àRENT du contenu (drafts, briefs, rapports). Make d√©place de la data entre outils, il ne cr√©e rien. | Nos agents sont plug-and-play pour non-tech. n8n est un outil de dev. |

**Pourquoi Dust/Lindy ne font pas √ßa (m√™me raison qu'Outreach ne faisait pas les images personnalis√©es) :**

Les incumbents ne construisent pas la feature parce qu'ils ne sont pas au m√™me endroit :

1. **Dust construit pour le CTO/admin IT** ‚Äî observability, s√©curit√©, agent chaining. Leur buyer veut de la gouvernance, pas un daily briefing op√©rationnel.
2. **Lindy construit pour des √©quipes business SMB qui savent d√©j√† ce qu'elles veulent automatiser** ‚Äî l'user choisit un template, le configure, le d√©ploie. Leur buyer est tech-aware : il sait ce qu'est un workflow, il sait quel process automatiser. Lindy ne lui dit pas ce qu'il rate.
3. **Zapier/Make/n8n construisent de la plomberie** ‚Äî ils connectent A √† B. Ils ne comprennent pas le contexte business et ne g√©n√®rent rien.
4. **Personne ne construit pour Sophie, Claire, Antoine, Julien** ‚Äî les responsables de PME qui ne savent pas ce qu'ils ratent, qui ne savent pas ce qu'est un "trigger" ou un "workflow", et qui n'iront jamais configurer un agent from scratch. Thomas ne sait pas que 3 deals dorment. Julien ne sait pas qu'un ticket premium est non assign√© depuis 2h. Antoine ne sait pas que 8 CV attendent depuis 5 jours. Sophie ne sait pas que ‚Ç¨12K de factures sont overdue.

C'est une **contrainte de positionnement**, pas technique.

**Ce qu'on apprend de chaque concurrent :**

Ces concurrents sont des machines de guerre. Chacun a r√©solu un probl√®me mieux que nous ne le ferons Day 1. L'intelligence c'est de voler les bonnes id√©es et de les int√©grer dans notre produit.

| Concurrent | Ce qu'ils font mieux que tout le monde | Ce qu'on int√®gre chez nous |
|------------|---------------------------------------|---------------------------|
| **Lindy** | **Natural language agent creation.** L'user d√©crit ce qu'il veut en fran√ßais, l'agent se construit. Pas de drag-and-drop obligatoire ‚Äî juste "je veux un agent qui relance mes leads apr√®s 3 jours." Templates de tr√®s haute qualit√© (1,000+) qui marchent out of the box. Agent Swarms (clonage parall√®le pour ex√©cuter 500 t√¢ches en simultan√©). Autopilot (Computer Use ‚Äî l'agent navigue sur le web quand il n'y a pas d'API). | **V1 :** Templates plug-and-play qui marchent imm√©diatement (m√™me standard de qualit√©). Natural language customization dans React Flow ("ajoute une condition : seulement si le deal > ‚Ç¨10K" / "escalade le ticket si le client est Premium" / "ne screene que les CV avec 3+ ans d'exp√©rience"). **V2 :** Agent Swarms pour l'outbound massif. Computer Use pour les outils sans API. |
| **Dust** | **Permissions granulaires** ‚Äî les docs engineering ne fuitent pas vers sales. SSO/SCIM, audit logs, data residency. **Knowledge grounding** ‚Äî les agents sont ancr√©s dans les donn√©es de l'entreprise (Notion, Slack, GitHub, Confluence), pas juste les APIs. $7.3M ARR avec 66 personnes (>$110K ARR/employ√©). 70% adoption hebdo chez des entreprises de 3,000+ personnes. **"Builder community"** interne ‚Äî les tinkerers de l'entreprise cr√©ent des agents pour leur √©quipe. | **V1 :** Knowledge base connect√©e (l'agent sait ce que l'entreprise sait, pas juste ce qu'il y a dans le CRM). **V1.1 :** Permissions par r√¥le (le commercial ne voit pas les donn√©es RH). **V2 :** "Builder" interne ‚Äî les power users de la PME cr√©ent des agents pour leur √©quipe et les partagent. |
| **Zapier** | **8,000+ int√©grations** ‚Äî la couverture la plus large du march√©. **Documentation exemplaire** ‚Äî chaque int√©gration a des guides, exemples, templates. **Reliability** ‚Äî 81 milliards de t√¢ches ex√©cut√©es, √ßa tourne. Utilis√© par 69% du Fortune 1000. **Copilot AI** (ZapConnect 2025) ‚Äî d√©crire une automation en langage naturel et elle se construit. **Template library** ‚Äî des milliers de templates pr√©-faits class√©s par use case, avec un moteur de recherche. $400M de revenue projet√© en 2025, bootstrapped. | **V1 :** Template library searchable avec filtres par m√©tier, outil connect√©, et cas d'usage (copier la logique de classification de Zapier). Documentation par template : chaque agent a sa page "comment √ßa marche, ce qu'il connecte, exemples de r√©sultats." **Toujours :** Fiabilit√© comme obsession ‚Äî monitoring, alerting, retry. Si l'agent plante, l'user perd confiance d√©finitivement. |
| **Make** | **√âditeur visuel best-in-class** ‚Äî le canvas Make est le plus intuitif du march√© pour visualiser des workflows complexes. On voit le flow de donn√©es, les branches, les conditions. **Text aggregation** ‚Äî excellente capacit√© √† combiner des donn√©es de sources multiples en un output structur√©. **MCP Server** ‚Äî les sc√©narios sont modularisables comme des outils r√©utilisables par d'autres agents (scalabilit√© enterprise). **Make Grid** ‚Äî carte auto-g√©n√©r√©e de tout le paysage d'automation de l'entreprise (quels agents, connect√©s √† quoi, quel flux de donn√©es). | **V1 :** S'inspirer du canvas Make pour notre √©diteur React Flow ‚Äî les nodes doivent montrer le flux de donn√©es en temps r√©el, pas juste des bo√Ætes statiques. **V1.1 :** Vue "Grid" ‚Äî un dashboard qui montre tous les agents actifs, leurs connexions, et les flux de donn√©es entre eux. **V2 :** Templates modulaires r√©utilisables (un agent qui enrichit un lead peut √™tre r√©utilis√© dans 10 workflows diff√©rents). |
| **n8n** | **Open-source + self-hosted** ‚Äî data sovereignty totale. **Code Node** ‚Äî possibilit√© de drop en JavaScript/Python √† n'importe quel point du workflow (plafond de complexit√© illimit√©). **AI-native** ‚Äî 70+ nodes LangChain, support RAG natif, vector databases. **Git version control** sur les workflows ‚Äî rollback, branching, collaboration. **Debugging avanc√©** ‚Äî pinned/mock data, global error triggers, logs d√©taill√©s. Community tr√®s active qui cr√©√© des nodes custom. | **V1 :** Logs d'ex√©cution d√©taill√©s et debugging visible pour chaque agent (l'user voit exactement ce qui s'est pass√© quand un agent √©choue). **V1.1 :** Possibilit√© d'ajouter du code custom dans un node React Flow (pour les power users qui veulent aller plus loin que les templates). **V2 :** Export/import de workflows + versioning. |

### 2.2 Ce que Lindy n'a pas ‚Äî Nos 2 features diff√©renciantes

**Ce ne sont pas des "wedges" d√©finitifs.** Ce sont 2 features que Lindy n'a pas et qui nous diff√©rencient au launch. Le vrai wedge √©merge en M1-M3 via l'usage ‚Äî peut-√™tre que c'est une de ces 2, peut-√™tre que c'est autre chose.

**Feature 1 : Le Scan ‚Äî "Connecte tes outils, on te montre ce qui tombe entre les mailles"**

Lindy te demande de choisir un template et de le configurer. Nous, on scanne tes outils et on te montre imm√©diatement ce qui n√©cessite ton attention ‚Äî adapt√© aux outils que TU utilises :

> **Sales** ‚Äî 3 deals (‚Ç¨47K) sans activit√© depuis 7+ jours. Drafts relance pr√™ts.
> **Support** ‚Äî 5 tickets proches du SLA, dont 1 client premium non assign√©.
> **Marketing** ‚Äî Campagne "Webinar F√©v" : open rate en chute. 3 MQLs non transmis.
> **HR** ‚Äî 6 candidatures non trait√©es depuis >48h.
> **Finance** ‚Äî ‚Ç¨12K de factures overdue. 2 paiements Stripe √©chou√©s.
> **Projets** ‚Äî 8 t√¢ches overdue, 3 sans assign√© avec deadline demain.
>
> **[Traiter les priorit√©s] [Activer les agents pour le reste]**

L'user n'a pas besoin de savoir ce qu'est un agent. Il voit ce qui tombe entre les mailles et clique pour r√©soudre. Le scan s'√©largit automatiquement quand il connecte un nouvel outil.

**Feature 2 : Le Style Learner ‚Äî "C'est MON style"**

Lindy produit des drafts g√©n√©riques. Chez nous, chaque correction de l'user est captur√©e. En 2 semaines, les drafts ressemblent √† ce que l'user aurait √©crit lui-m√™me. √áa fonctionne sur tous les outputs : emails, messages Slack, r√©sum√©s, rapports, r√©ponses support.

**V√©rification des 5 propri√©t√©s Lemlist :**
1. ‚úÖ **Visible en 5 secondes** ‚Äî "3 deals dormants, 5 tickets proches SLA, 6 CV non trait√©s, ‚Ç¨12K factures overdue" + actions pr√™tes
2. ‚úÖ **R√©sultat imm√©diat** ‚Äî tu envoies le draft, le ticket est trait√©, le deal avance, le candidat est relanc√©, la facture est suivie, la t√¢che est assign√©e
3. ‚úÖ **Partageable** ‚Äî "regarde ce que l'outil a trouv√© dans notre bo√Æte"
4. ‚úÖ **R√©sout un vrai probl√®me quotidien** ‚Äî chaque m√©tier a ses t√¢ches qui tombent entre les mailles
5. ‚úÖ **Simple √† comprendre** ‚Äî pas besoin de savoir ce qu'est un "agent"

Et surtout : **li√© au CA.** Le draft r√©active un deal qui dormait. Le ticket critique est trait√© avant le breach SLA. Le candidat qualifi√© ne tombe pas entre les mailles. La facture overdue est relanc√©e. La t√¢che bloquante est reassign√©e. Mesurable en euros sur chaque m√©tier.

### 2.3 Le Scan ‚Äî Transversal, stress-test√©, pas de bullshit

Le scan n'est pas juste l'onboarding. C'est le produit. Il tourne TOUS LES JOURS, sur TOUS les m√©tiers connect√©s.

**Principe fondamental : un seul faux positif embarrassant tue la confiance pour toujours.** Mieux vaut 4 alertes justes que 8 alertes dont 3 sont fausses. Chaque signal ci-dessous a √©t√© v√©rifi√© contre les APIs r√©elles pour confirmer que la donn√©e existe et est fiable.

**Le scan s'adapte aux outils connect√©s.** Si la PME connecte son CRM ‚Üí signaux sales. Si elle connecte Zendesk ‚Üí signaux support. Si elle connecte les deux ‚Üí vue crois√©e. Pas de scan g√©n√©rique unique, mais un scan qui se construit √† partir de ce que l'entreprise utilise r√©ellement.

---

**SALES ‚Äî CRM (HubSpot, Pipedrive, Salesforce) + Email (Gmail/Outlook) + Calendar**

| Signal | Source technique | Fiabilit√© |
|--------|-----------------|-----------|
| Deals sans activit√© >X jours | CRM : `notes_last_updated` (HubSpot) / `last_activity_id` (Pipedrive). Un appel API. | **95%+** |
| Leads entrants non trait√©s >24h | Email : threads entrants sans r√©ponse. CRM : email non pr√©sent dans les contacts. | **90%+** |
| Emails envoy√©s sans r√©ponse >48h | Email : `threads.get` ‚Üí dernier message = le n√¥tre ‚Üí delta timestamp. Cross-check CRM. | **85%** |
| Temps de r√©ponse en hausse (tendance 4-6 sem) | Email : timestamps des messages dans les threads. Moyenne glissante. | **80%** |
| Fr√©quence d'√©change en baisse (tendance 4-6 sem) | Email : comptage de messages par p√©riode par contact. Moyenne glissante. | **80%** |
| Meeting annul√© sans replanification | Calendar API : event cancelled/declined + pas de nouvel event avec le m√™me contact. | **90%** |

**SUPPORT ‚Äî Zendesk, Freshdesk, Intercom, Crisp**

| Signal | Source technique | Fiabilit√© |
|--------|-----------------|-----------|
| Tickets ouverts sans r√©ponse >Xh | Ticket `status` + `created_at` + absence de commentaire agent. Donn√©e structurelle. | **95%+** |
| SLA en passe d'√™tre d√©pass√© | Zendesk : API `ticket_metric_events` type `breach`. Freshdesk : `due_by` field. | **95%+** |
| Tickets non assign√©s | `assignee_id` = null sur ticket ouvert. Signal binaire. | **95%+** |
| Temps de r√©solution moyen en hausse | Calcul sur `solved_at - created_at` glissant 4 semaines. | **85%** |
| Tickets r√©ouverts (indicateur insatisfaction) | Ticket pass√© de `solved` √† `open`. Comptage sur p√©riode. | **90%** |

**MARKETING ‚Äî HubSpot Marketing, Mailchimp, Google Ads, Meta Ads**

| Signal | Source technique | Fiabilit√© |
|--------|-----------------|-----------|
| Campagnes email avec taux d'ouverture en chute | Mailchimp/HubSpot : `open_rate` par campagne vs. moyenne historique. | **85%** |
| Leads MQL non transmis √† sales >48h | HubSpot : `lifecyclestage` = MQL + pas de deal associ√© + delta > 48h. | **90%** |
| Budget ads d√©pens√© sans conversion | Google Ads/Meta Ads : `cost` > seuil + `conversions` = 0 sur X jours. | **90%** |
| Landing pages avec taux de bounce anormal | Google Analytics : `bounce_rate` par page vs. moyenne. | **80%** |

**HR & RECRUTEMENT ‚Äî Workable, Welcome to the Jungle, BambooHR, Lever**

| Signal | Source technique | Fiabilit√© |
|--------|-----------------|-----------|
| Candidatures non trait√©es >48h | Application `status` = new + `created_at` > 48h. Donn√©e structurelle. | **95%+** |
| Offres envoy√©es sans r√©ponse >X jours | Offer `status` = sent + delta. | **90%** |
| Postes ouverts depuis >X jours sans shortlist | Job `created_at` + 0 candidats en phase interview. | **85%** |

**OPS & FINANCE ‚Äî Pennylane, QuickBooks, Stripe, Xero**

| Signal | Source technique | Fiabilit√© |
|--------|-----------------|-----------|
| Factures impay√©es >X jours | Invoice `status` = unpaid/overdue + `due_date`. Donn√©e structurelle. | **95%+** |
| Abonnements en churning | Stripe : `subscription.status` = `past_due` ou `canceled`. | **95%+** |
| Paiements r√©currents √©chou√©s | Stripe : `charge.failed` events. | **95%+** |

**PRODUCTIVIT√â & PROJETS ‚Äî Asana, Monday, Notion, Trello**

| Signal | Source technique | Fiabilit√© |
|--------|-----------------|-----------|
| T√¢ches overdue | Task `due_date` < today + `completed` = false. Signal binaire. | **95%+** |
| Projets sans mise √† jour >X jours | Project `modified_at` ou derni√®re t√¢che compl√©t√©e. | **90%** |
| T√¢ches sans assign√© avec deadline proche | Task `assignee` = null + `due_date` < 3 jours. | **95%+** |

---

**Pattern commun : tous ces signaux sont du metadata structurel.** Dates, statuts, compteurs, deltas. Pas de lecture de contenu, pas d'interpr√©tation LLM pour la d√©tection. Le LLM intervient UNIQUEMENT pour g√©n√©rer des drafts/actions contextuels.

**Signaux RETIR√âS du V1 apr√®s stress-test :**

| Signal retir√© | Raison |
|---------------|--------|
| ~~Promesses non tenues (email)~~ | N√©cessite lecture du body (trust barrier). Deadlines souvent vagues. V√©rification impossible. **~60-70% de fiabilit√© = inacceptable.** |
| ~~Longueur des emails~~ | `sizeEstimate` Gmail ‚â† longueur du texte √©crit (inclut headers, quoted text, signature, PJ). |
| ~~Handoffs bloqu√©s~~ | Forward = nouveau thread (invisible). Trop de faux positifs. |
| ~~Score de confiance sur drafts~~ | Aucune ground truth. Chiffre invent√©. |

---

**Scan initial (onboarding, 2 min) ‚Äî adapt√© aux outils connect√©s :**

Exemple PME avec CRM (HubSpot) + Support (Zendesk) + Facturation (Stripe) + Projets (Asana) + Marketing (Mailchimp) + HR (Workable) :

> **Sales ‚Äî 3 deals (‚Ç¨47K) sans activit√© depuis 7+ jours**
> - Acme Corp (‚Ç¨23K) ‚Äî derni√®re activit√© il y a 12 jours. **[Draft relance pr√™t]**
> - Beta SA (‚Ç¨15K) ‚Äî email envoy√© il y a 11 jours, pas de r√©ponse. **[Draft pr√™t]**
> - Gamma (‚Ç¨9K) ‚Äî aucune activit√© depuis 8 jours. **[Draft pr√™t]**
>
> **Support ‚Äî 5 tickets proches du SLA**
> - Ticket #1204 (client premium) ‚Äî SLA first reply dans 2h, pas encore assign√©. **[Assigner + Draft r√©ponse]**
> - 4 tickets ouverts >24h sans r√©ponse agent. **[Voir les tickets]**
>
> **Marketing ‚Äî 2 alertes**
> - Campagne "Webinar F√©vrier" : open rate 12% (vs. 28% moyenne). **[Voir la campagne]**
> - 3 MQLs non transmis √† sales depuis >48h. **[Transmettre + Draft intro]**
>
> **HR ‚Äî 6 candidatures non trait√©es**
> - Poste "Dev Frontend Senior" : 6 CV re√ßus >48h sans screening. **[Voir les candidats]**
>
> **Finance ‚Äî 3 factures impay√©es (‚Ç¨12K)**
> - Facture #2024-089 (‚Ç¨5K) ‚Äî overdue 14 jours. **[Draft relance paiement]**
> - 2 paiements Stripe √©chou√©s ce mois. **[Voir les d√©tails]**
>
> **Projets ‚Äî 8 t√¢ches overdue cette semaine**
> - 3 t√¢ches sans assign√© avec deadline demain. **[Voir dans Asana]**
>
> **[Traiter les priorit√©s] [Voir le dashboard complet]**

**Daily briefing (chaque matin, 8h) ‚Äî par persona :**

Pour Thomas (Sales) :
> üî¥ 2 deals √† relancer. 1 lead non trait√© depuis 6h. Drafts pr√™ts.
> üü° 2 meetings aujourd'hui. Briefs pr√©par√©s.

Pour Julien (Support) :
> üî¥ 1 ticket premium proche SLA. 3 tickets non assign√©s.
> üìä Temps de r√©solution moyen cette semaine : 4.2h (vs. 3.1h semaine derni√®re).

Pour Claire (Marketing) :
> üî¥ 2 MQLs non transmis √† sales depuis 3 jours. Campagne "Webinar F√©v" : open rate en chute de 15%.
> üü° ‚Ç¨350 d√©pens√©s sur Google Ads sans conversion depuis 8 jours.

Pour Antoine (HR) :
> üî¥ 6 candidatures non trait√©es depuis >48h sur "Dev Frontend Senior."
> üü° 1 offre envoy√©e sans r√©ponse depuis 4 jours. Poste "Chef de Projet" ouvert depuis 45 jours sans shortlist.

Pour Sophie (CEO) :
> üî¥ ‚Ç¨12K de factures overdue. 3 deals dormants. 5 tickets proches SLA. 6 CV non trait√©s.
> üìä Vue consolid√©e : 23 alertes trait√©es / 28 cette semaine.

**100% de la d√©tection est metadata-only.** Le LLM intervient UNIQUEMENT pour la g√©n√©ration de drafts et actions contextuels.

### 2.4 Les ~93 Templates ‚Äî Le catalogue complet

Comme Lemlist V1 √©tait un outil de prospection COMPLET (pas un prototype avec une seule feature), on lance avec un catalogue complet. Lindy a 1,000+ templates. On vise ~93 au launch ‚Äî suffisant pour couvrir tous les m√©tiers d'une PME. Avec Claude Code, chaque template = quelques heures de dev (trigger + fetch donn√©es via Pipedream + prompt + eval rules + workflow React Flow).

**Principe dogfooding :** Les agents prioritaires sont ceux que Martin et Ombeline utilisent pour g√©rer leur propre bo√Æte. Ils sont battle-tested avant d'√™tre propos√©s aux users.

**üí∞ SALES (22 templates) :**

| Template | Description |
|----------|-------------|
| Lead Qualifier | Qualification de leads (crit√®res BANT) |
| Sales Follow-up | S√©quences de suivi commercial |
| Sales Meeting Recorder | Enregistrement et r√©sum√© de r√©unions commerciales |
| Lead Generator | G√©n√©ration de leads |
| Lead Outreacher | Outreach personnalis√© |
| Outbound Phone Call Agent | Appels sortants automatis√©s |
| Enrich New Leads | Enrichissement de leads (LinkedIn, web, firmographics) |
| LinkedIn Personalized Message Drafter | Messages LinkedIn personnalis√©s |
| AI Sales Development Representative | SDR automatis√© complet |
| Contact Finder Info | Recherche de contacts et coordonn√©es |
| New Lead Qualifier | Qualification de nouveaux leads entrants |
| Case Study Drafter | R√©daction d'√©tudes de cas √† partir des deals gagn√©s |
| Email Finder | Recherche d'emails professionnels |
| Sales Call Prep | Pr√©paration d'appels commerciaux avec contexte relation |
| Sales Coach | Coaching commercial post-call (talk ratio, objections, next steps) |
| HubSpot Contact Assistant | Assistant contacts HubSpot (enrichissement, nettoyage) |
| Email Negotiator | N√©gociation par email avec suggestions strat√©giques |
| In-depth Lead Researcher | Recherche approfondie de leads (company, persona, pain points) |
| Proposal Drafter | R√©daction de propositions commerciales |
| Inbound Sales Agent | Agent commercial entrant (qualification + routing) |
| ICP Insights Miner | Analyse de profil client id√©al √† partir des deals gagn√©s |
| Sales Insights | Dashboard insights commerciaux (patterns, trends, recommandations) |

**üì¢ MARKETING (20 templates) :**

| Template | Description |
|----------|-------------|
| Brand Monitor | Surveillance de marque (mentions, sentiment, concurrents) |
| Newsletter Writer | R√©daction de newsletters √† partir du contenu r√©cent |
| AI CMO Creative Agent | Cr√©ation de contenus marketing multi-format |
| AI CMO Research Agent | Recherche marketing (trends, audience, competitors) |
| AI CMO Analysis Agent | Analyse marketing (ROI campagnes, attribution, recommandations) |
| SEO Blog Writer | Articles de blog optimis√©s SEO |
| Content Repurposing Agent | Recyclage de contenu multi-plateformes (blog ‚Üí LinkedIn ‚Üí Twitter ‚Üí newsletter) |
| SEO Audit Agent | Audit SEO complet avec recommandations |
| SEO Assistant | Assistant SEO continu (keywords, backlinks, ranking) |
| Turn Podcasts into Blog Posts | Conversion podcast ‚Üí article de blog |
| Partnership Collaboration Scout | Recherche de partenariats pertinents |
| Marketing Focus Group | Groupe de focus virtuel (simulation audience) |
| Copywriting Assistant | Assistant copywriting avec ton de marque |
| Influencer Outreach | Outreach influenceurs personnalis√© |
| Press Release Drafter | R√©daction de communiqu√©s de presse |
| Support Inbox Content Creator | Cr√©ation de contenu FAQ/help center depuis les tickets support |
| Newsletters Into Twitter Content | Conversion newsletter ‚Üí threads Twitter |
| Content Writer | R√©dacteur de contenu long format |
| Social Media Manager | Gestionnaire r√©seaux sociaux (planning, cr√©ation, scheduling) |

**üéß SUPPORT (21 templates) :**

| Template | Description |
|----------|-------------|
| Customer Support Email Responder | R√©ponses email automatis√©es avec contexte client |
| SMS Support Bot | Support par SMS |
| WhatsApp Support Agent | Support WhatsApp |
| Email Triager | Tri intelligent d'emails (urgence, cat√©gorie, routing) |
| Email Responder | R√©ponses automatiques personnalis√©es |
| Phone Support Agent | Support t√©l√©phonique IA |
| Website Customer Support | Chatbot site web avec knowledge base |
| Support Slackbot | Bot Slack interne pour questions √©quipe |
| AI Receptionist | R√©ceptionniste IA (accueil, routing, FAQ) |
| Knowledge Retrieval | R√©cup√©ration intelligente de connaissances internes |
| Daily Support Email Report | Rapport quotidien support (volume, SLA, satisfaction) |
| Daily Slack Digest | R√©sum√© quotidien des conversations Slack importantes |
| Urgent Ticket Alert Agent | Alertes temps r√©el sur tickets critiques |
| Support Ticket Dispatcher | Dispatch intelligent de tickets (comp√©tences, charge, urgence) |
| Feedback Survey Agent | Collecte de feedback post-interaction |
| Customer Sentiment Tracker | Suivi du sentiment client dans le temps |
| Support FAQ Generator | G√©n√©ration automatique de FAQ √† partir des tickets r√©currents |
| Support Bot with Human Handoff | Bot avec escalade humaine intelligente |
| Query Your Files | Interrogation de documents internes en langage naturel |
| Telegram Bot | Bot Telegram support |
| AI Customer Calls Rep | Repr√©sentant appels clients IA |

**üî¨ RESEARCH / PRODUCT (14 templates) :**

| Template | Description |
|----------|-------------|
| Voice of the Customer | Analyse voix du client (patterns, sentiment, besoins) |
| Competition Tracker | Veille concurrentielle automatis√©e |
| Web Researcher | Recherche web avanc√©e sur un sujet |
| Web Monitoring | Surveillance de sites web (changements, prix, contenu) |
| Daily Product Updates | MAJ produit quotidiennes (GitHub commits, PRs, issues) |
| User Research Notetaker | Notes structur√©es de recherche utilisateur |
| Design Critique Summarizer | R√©sum√© de critiques design |
| Disseminate Meeting Insights | Partage automatique d'insights r√©unions aux bonnes personnes |
| User Feedback Tracker | Suivi et cat√©gorisation du feedback utilisateur |
| Product Documentation Creator | Cr√©ation de documentation produit |
| Bug Triage & Prioritization | Tri et priorisation automatique de bugs |
| Daily Product Feedback Email Report | Rapport quotidien feedback produit |
| Product Documentation Q&A Agent | Q&A sur la documentation produit |
| User Feedback Collector | Collecteur de feedback multi-canal |

**üë• HR & RECRUITING (15 templates) :**

| Template | Description |
|----------|-------------|
| Recruiting Agent | Agent de recrutement complet (sourcing ‚Üí screening ‚Üí scheduling) |
| Resume Screening Agent | Filtrage automatis√© de CV (crit√®res configurables) |
| Company Knowledge Base | Base de connaissances entreprise interrogeable |
| Employee Onboarding Assistant | Assistant onboarding (checklist, docs, follow-ups) |
| Hiring Team Sync Summary | R√©sum√© synchronisation √©quipe recrutement |
| Candidate Evaluation Agent | √âvaluation structur√©e de candidats |
| Candidate Screener | Screening rapide de candidats |
| AI Interview Answer Generator | G√©n√©rateur de grilles d'√©valuation entretien |
| Resume Data Extractor & Organizer | Extraction structur√©e de donn√©es CV |
| Interview Questions Generator | G√©n√©rateur de questions d'entretien adapt√©es au poste |
| Candidate Background Researcher | Recherche background candidat (LinkedIn, publications, projets) |
| Job Description Optimizer | Optimisation d'offres d'emploi (inclusivit√©, attractivit√©, SEO) |
| Offer Letter Generator | G√©n√©rateur de lettres d'offre personnalis√©es |
| Offer Negotiation Assistant | Assistant n√©gociation offres |
| HR Policy Bot | Bot politique RH (cong√©s, avantages, process) |

**‚öôÔ∏è OPERATIONS (7 templates) :**

| Template | Description |
|----------|-------------|
| AI Todos Manager | Gestionnaire de t√¢ches IA (priorisation, rappels, redistribution) |
| Vendor Invoice & Payment Tracker | Suivi factures fournisseurs et paiements |
| Project Status Updater | MAJ automatique statut projet |
| Daily Ops Digest | R√©sum√© op√©rations quotidien |
| Meeting Agenda & Follow-up | Agenda pr√©-meeting + suivi post-meeting |
| Overdue Task Nudger | Rappels intelligents t√¢ches en retard |
| Inventory Low-Stock Alert | Alertes stock bas |

**üìÖ PRODUCTIVITY (3 templates) :**

| Template | Description |
|----------|-------------|
| Meeting Scheduler | Planification intelligente de r√©unions (disponibilit√©s, fuseaux) |
| Email Assistant | R√©daction et gestion d'emails (triage, drafts, templates) |
| Task Manager | Organisation des t√¢ches et projets |

**üõ†Ô∏è CUSTOM (1 template) :**

| Template | Description |
|----------|-------------|
| Custom Agent | Agent personnalisable ‚Äî l'user d√©finit trigger, sources, prompt, actions via React Flow |

**üìä R√©sum√© par cat√©gorie :**

| Cat√©gorie | Templates | Personas cibles |
|-----------|-----------|----------------|
| Sales | 22 | Thomas (Sales Director) |
| Support | 21 | Julien (Head of Support) |
| Marketing | 20 | Claire (Responsable Marketing) |
| HR & Recruiting | 15 | Antoine (DRH) |
| Research / Product | 14 | Product Manager, CEO |
| Operations | 7 | Sophie (Head of Ops) |
| Productivity | 3 | Tous |
| Custom | 1 | Power users |
| **TOTAL** | **~93** | |

**Ce qui fait la diff√©rence vs Lindy (1,000 templates) :**

1. **Le scan comme hook** ‚Äî "3 deals dormants, 5 tickets proches SLA, 6 CV non trait√©s, ‚Ç¨12K factures overdue" avant m√™me d'activer un agent. Lindy n'a pas √ßa.
2. **Chaque agent a le Style Learner** ‚Äî s'am√©liore avec les corrections de l'utilisateur.
3. **Chaque agent est dogfood√©** ‚Äî test√© sur une vraie bo√Æte avec des vrais r√©sultats document√©s.
4. **Plug-and-play pour non-tech** ‚Äî pas de workflow √† configurer from scratch. Connecte tes outils, choisis un template, customise si tu veux via React Flow.
5. **Couverture multi-m√©tier** ‚Äî sales, marketing, support, HR, ops, product. Pas juste sales.

**Extension V1.1+ :** Guid√©e par le feedback users. Quand Thomas demande un template qu'on n'a pas, c'est le signal pour le construire.

### 2.5 Eval Layer ‚Äî Confiance visible

**Pourquoi c'est un diff√©renciateur :** Lindy g√©n√®re un draft, tu ne sais pas pourquoi il dit ce qu'il dit. Tu ne sais pas si l'agent est s√ªr de lui ou s'il invente. Tu dois tout relire √† chaque fois. Chez nous, chaque output d'agent passe par 3 couches d'√©valuation et le user VOIT le r√©sultat. C'est la diff√©rence entre "un assistant dont tu ne sais jamais s'il a bien fait" et "un assistant qui te dit quand il doute."

#### L1 ‚Äî Assertions (checks d√©terministes)

Des r√®gles cod√©es en dur, ex√©cut√©es apr√®s chaque output. Z√©ro LLM, <10ms, 100% fiables.

**Assertions par type d'agent :**

| Cat√©gorie | Assertion | Fail ‚Üí |
|-----------|-----------|--------|
| **Tout output texte** | Contient le nom du destinataire/contact | ‚ùå Output bloqu√©, r√©g√©n√©r√© |
| **Tout output texte** | Ne contient PAS de placeholder type [NOM], [ENTREPRISE], {variable} | ‚ùå Output bloqu√© |
| **Tout output texte** | Longueur dans la range attendue (configurable par agent) | ‚ö†Ô∏è Warning visible |
| **Tout output texte** | Pas de contenu en anglais si user en fran√ßais (et vice versa) | ‚ö†Ô∏è Warning |
| **Tout output texte** | Pas de donn√©es d'un autre contact/deal/ticket dans l'output (cross-contamination) | ‚ùå Output bloqu√©, alert critique |
| **Sales : Follow-Up / Deal Revival** | R√©f√©rence un √©change r√©el (date ou sujet extrait des donn√©es fetch√©es) | ‚ùå Draft bloqu√© si aucune r√©f√©rence |
| **Sales : CRM Update** | Le champ √† modifier existe dans le CRM connect√© | ‚ùå Action bloqu√©e |
| **Sales : Outbound** | L'email du destinataire est valide (regex + MX check) | ‚ùå Action bloqu√©e |
| **Support : Ticket Response** | R√©f√©rence le num√©ro du ticket et le sujet original | ‚ùå Draft bloqu√© |
| **Support : Ticket Response** | Ne propose pas une action que l'agent n'a pas le droit de faire (ex: remboursement) | ‚ö†Ô∏è Warning |
| **HR : Candidate Reply** | R√©f√©rence le poste et le nom du candidat | ‚ùå Draft bloqu√© |
| **HR : Candidate Reply** | Pas de mention de salaire sauf si explicitement dans la config | ‚ö†Ô∏è Warning |
| **Marketing : Content** | Respecte le word count cible du format (tweet ‚â† blog post) | ‚ö†Ô∏è Warning |
| **Finance : Invoice Follow-Up** | Montant et date d'√©ch√©ance correspondent √† la facture dans Stripe/QuickBooks | ‚ùå Draft bloqu√© si montant incorrect |
| **Finance : Invoice Follow-Up** | Inclut le lien de paiement si disponible | ‚ö†Ô∏è Warning |
| **Ops : Meeting Prep** | Liste au moins 1 participant et 1 point d'agenda | ‚ùå Bloqu√© si vide |

**Impl√©mentation :** Chaque template d'agent d√©clare ses assertions dans sa config. Le moteur d'ex√©cution les run automatiquement. Un assert fail = le draft ne passe pas √† L2.

```typescript
eval_assertions: [
  { check: "contains_recipient_name", severity: "block" },
  { check: "no_placeholders", severity: "block" },
  { check: "references_real_exchange", severity: "block" },
  { check: "word_count_range", min: 50, max: 500, severity: "warn" },
  { check: "correct_language", severity: "warn" },
  { check: "no_cross_contamination", severity: "block" }
]
```

#### L2 ‚Äî Score de confiance (rule-based, 0-100)

Un score composite calcul√© SANS LLM, bas√© sur des m√©triques objectives.

**Composantes du score :**

| Signal | Poids | Comment c'est calcul√© | Exemple |
|--------|-------|----------------------|---------|
| **Historique de validation** | 35% | % des N derniers outputs similaires envoy√©s sans modification | 8/10 derniers outputs envoy√©s tels quels ‚Üí 80/100 |
| **Compl√©tude des donn√©es** | 25% | % des sources de donn√©es que l'agent a r√©ussi √† fetcher | Source 1 ‚úÖ + Source 2 ‚úÖ + Source 3 ‚ùå (token expir√©) ‚Üí 67/100 |
| **Fra√Æcheur des donn√©es** | 20% | √Çge des donn√©es les plus r√©centes utilis√©es | Derni√®re donn√©e: il y a 2h ‚Üí 95/100. Derni√®re donn√©e: il y a 30j ‚Üí 30/100 |
| **Assertions L1 pass√©es** | 20% | Ratio assertions passed vs warnings | 5/5 passed, 0 warnings ‚Üí 100/100. 4/5 passed, 1 warning ‚Üí 80/100 |

**Score composite :** `confidence = (historique √ó 0.35) + (compl√©tude √ó 0.25) + (fra√Æcheur √ó 0.20) + (assertions √ó 0.20)`

**Ce que l'user voit (adapt√© selon l'agent) :**

Sales Follow-Up :
```
Score: 91/100  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë
‚îú‚îÄ‚îÄ Historique: 94% (vos 8 derniers outputs similaires envoy√©s sans modif)
‚îú‚îÄ‚îÄ Donn√©es: 100% (Gmail ‚úÖ HubSpot ‚úÖ Calendar ‚úÖ)
‚îú‚îÄ‚îÄ Fra√Æcheur: 88% (derni√®res donn√©es: il y a 4h)
‚îî‚îÄ‚îÄ Checks: 80% (5/5 pass√©s, 1 warning: longueur)
```

Resume Screening :
```
Score: 88/100  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë
‚îú‚îÄ‚îÄ Historique: 85% (vos 6 derniers screenings approuv√©s sans modif)
‚îú‚îÄ‚îÄ Donn√©es: 100% (Workable ‚úÖ Job Description ‚úÖ)
‚îú‚îÄ‚îÄ Fra√Æcheur: 95% (derni√®res donn√©es: il y a 1h)
‚îî‚îÄ‚îÄ Checks: 80% (4/4 pass√©s, 1 warning: candidat senior)
```

Invoice Follow-Up :
```
Score: 94/100  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë
‚îú‚îÄ‚îÄ Historique: 92% (vos 10 derni√®res relances envoy√©es sans modif)
‚îú‚îÄ‚îÄ Donn√©es: 100% (Stripe ‚úÖ HubSpot ‚úÖ)
‚îú‚îÄ‚îÄ Fra√Æcheur: 98% (derni√®res donn√©es: il y a 30min)
‚îî‚îÄ‚îÄ Checks: 100% (5/5 pass√©s, montant v√©rifi√©, lien paiement inclus)
```

**Thresholds d'action :**

| Score | Couleur | Comportement |
|-------|---------|-------------|
| 85-100 | üü¢ Vert | "Pr√™t √† envoyer." Si mode auto activ√© ‚Üí envoi automatique. |
| 60-84 | üü° Jaune | "V√©rifiez avant d'envoyer." Toujours en attente d'approbation. |
| 40-59 | üü† Orange | "Confiance faible ‚Äî donn√©es incompl√®tes ou obsol√®tes." Draft affich√© mais flagg√©. |
| 0-39 | üî¥ Rouge | "Draft non fiable." Affich√© avec warning mais action d√©sactiv√©e. L'user doit r√©√©crire. |

#### L3 ‚Äî LLM-as-Judge (review avant actions irr√©versibles)

Un second appel LLM (Claude Haiku ‚Äî rapide et pas cher) qui review l'output avant qu'il soit pr√©sent√© √† l'user. Ce n'est PAS le m√™me LLM qui a g√©n√©r√© le draft.

**Quand L3 se d√©clenche :**
- Toute action irr√©versible (envoyer un email, r√©pondre √† un ticket, modifier le CRM, poster sur Slack, envoyer une offre candidat)
- Score L2 < 70 (le syst√®me doute, second avis)
- Premier output pour un nouveau contact/ticket/candidat (pas d'historique)
- L'agent d√©tecte un sujet sensible (r√©clamation, r√©siliation, conflit, candidature senior)

**Ce que L3 v√©rifie :**

| Check | Prompt simplifi√© | Fail ‚Üí |
|-------|-----------------|--------|
| **Coh√©rence de ton** | "Cet output est-il coh√©rent avec les 5 derniers outputs de ce user pour ce type d'agent ?" | ‚ö†Ô∏è Warning : "Le ton est plus formel que d'habitude" |
| **Hallucination check** | "Chaque fait mentionn√© est-il pr√©sent dans les donn√©es sources ci-dessous ?" | ‚ùå Block : "Le draft mentionne un meeting le 15 mars qui n'existe pas dans les donn√©es" |
| **Appropriateness** | "Cet output est-il appropri√© √©tant donn√© le contexte ?" | ‚ùå Block : exemples ‚Äî "Le contact a 3 tickets critiques ouverts, une relance commerciale serait mal per√ßue" / "Le candidat a refus√© l'offre, ne pas relancer sur le m√™me poste" |
| **Compl√©tude** | "L'output r√©pond-il √† l'objectif de l'agent ?" | ‚ö†Ô∏è Warning : "Le draft est trop vague, ne mentionne pas l'objet de la relance" |

**Co√ªt L3 :** Claude Haiku √† $0.80/1M tokens. Un eval L3 = ~500 tokens input + ~100 tokens output = ~$0.0005. Sur 1,000 evals/jour = $0.50/jour. N√©gligeable.

**Ce que l'user voit quand L3 intervient :**

```
‚ö†Ô∏è L3 Review ‚Äî 1 point d'attention (Sales Follow-Up) :
"Le draft mentionne 'comme convenu lors de notre call' mais aucun call
n'appara√Æt dans votre calendrier avec ce contact sur les 30 derniers jours.
V√©rifiez avant d'envoyer."
[Envoyer quand m√™me] [Modifier le draft]
```

```
‚ö†Ô∏è L3 Review ‚Äî 1 point d'attention (Support Ticket Reply) :
"Le draft propose un remboursement, mais votre policy limite les remboursements
aux commandes < 30 jours. Ce ticket concerne une commande de 45 jours."
[Envoyer quand m√™me] [Modifier le draft]
```

```
‚ö†Ô∏è L3 Review ‚Äî 1 point d'attention (Resume Screening) :
"Le screening rejette un candidat pour 'exp√©rience insuffisante' mais son CV
mentionne 4 ans chez Datadog en tant que Senior Frontend. V√©rifiez les crit√®res."
[Confirmer le rejet] [Revoir le candidat]
```

```
‚ö†Ô∏è L3 Review ‚Äî 1 point d'attention (Invoice Follow-Up) :
"La facture #2024-089 a un avoir en cours de traitement (‚Ç¨1,200). Le montant
de relance devrait √™tre ‚Ç¨3,800 et non ‚Ç¨5,000. Draft ajust√©."
[Envoyer le draft ajust√©] [Modifier]
```

#### Autonomie progressive ‚Äî de draft-only √† auto-pilot

L'eval layer ne sert pas qu'√† v√©rifier ‚Äî elle sert √† **construire la confiance progressivement** jusqu'√† l'autonomie.

| Phase | Condition | Comportement | L'user fait |
|-------|-----------|-------------|-------------|
| **Phase 1 : Draft-only** | Par d√©faut, semaines 1-2 | Tout est un draft en attente d'approbation. Rien ne part sans clic. | Review chaque draft, approuve/modifie/rejette |
| **Phase 2 : Batch approve** | 5+ validations sans modif pour un type d'agent | "Approuver tout" se d√©bloque. L'user peut valider 10 drafts en 1 clic. | Review rapide, approve en batch |
| **Phase 3 : Auto-send conditionnel** | 20+ validations <10% modif + score L2 > 85 | L'agent propose : "Activer l'envoi automatique pour cet agent quand le score > 85 ?" | L'user choisit quels agents passent en auto |
| **Phase 4 : Full auto** | 50+ validations, modif rate < 5%, z√©ro L3 block en 30j | L'agent agit seul pour les actions r√©versibles (draft ‚Üí send). Les actions irr√©versibles restent en approval. | L'user check le daily briefing, intervient sur les exceptions |

**Rollback automatique :** Si apr√®s passage en Phase 3-4, le taux de modification remonte > 30% ou un L3 block survient, l'agent revient automatiquement en Phase 2. Le user est notifi√© : "L'agent Follow-Up est revenu en mode approbation suite √† 3 modifications cons√©cutives."

**Le data model de l'eval :**

Chaque ex√©cution d'agent produit un `AgentRun` stock√© en DB :

```typescript
model AgentRun {
  id              String    // unique run ID
  agentTemplateId String    // quel agent template
  userId          String    // quel user
  triggeredAt     DateTime  // quand l'agent s'est d√©clench√©
  triggeredBy     String    // "cron" | "webhook" | "manual"

  // Donn√©es fetch√©es
  dataSources     Json      // { gmail: { status: "ok", count: 12 }, hubspot: { status: "ok", count: 3 } }

  // Output
  outputType      String    // "draft_email" | "draft_ticket_reply" | "crm_update" | "slack_message" | "alert" | "report" | "task_action" | "candidate_screening" | "invoice_followup" | "marketing_alert"
  outputContent   String    // le draft, l'action, ou le rapport
  llmModel        String    // "gemini-flash" | "haiku-3.5" | "sonnet-4"
  llmTokensUsed   Int       // pour tracking co√ªts

  // Eval L1
  l1Assertions    Json      // [{ check: "contains_name", passed: true }, ...]
  l1Passed        Boolean   // toutes les assertions block pass√©es ?

  // Eval L2
  l2Score         Int       // 0-100
  l2Breakdown     Json      // { history: 94, completeness: 100, freshness: 88, assertions: 80 }

  // Eval L3 (nullable ‚Äî pas toujours d√©clench√©)
  l3Triggered     Boolean
  l3Model         String?   // "gemini-flash"
  l3Checks        Json?     // [{ check: "hallucination", passed: true }, ...]
  l3Blocked       Boolean?
  l3Reason        String?   // "Draft mentionne un meeting inexistant"

  // User action
  userAction      String?   // "approved" | "modified" | "rejected" | "pending"
  userModifiedAt  DateTime?
  draftDiff       String?   // diff entre output original et version envoy√©e (pour Style Learner)

  // R√©sultat final
  finalAction     String?   // "sent" | "blocked" | "cancelled"
  finalAt         DateTime?
}

### 2.6 Style Learner ‚Äî Les outputs deviennent "les tiens"

1. Agent g√©n√®re un output (draft email, r√©ponse ticket, message Slack, rapport)
2. L'user modifie (raccourcit, change le ton, reformule, ajoute du contexte)
3. Diff captur√©
4. 10 derniers diffs inject√©s en few-shot
5. Prochain output est plus "toi"

R√©sultat : taux de modification ~70% (semaine 1) ‚Üí ~20% (mois 3).

C'est le compound advantage. M1 : agents √©quivalents √† la concurrence. M3 : meilleurs parce qu'ils ont appris. M6 : significativement meilleurs. Un concurrent qui arrive en M6 part de z√©ro.

### 2.7 Int√©grations ‚Äî Pipedream Connect + Agents autonomes

**Le principe :** On ne construit PAS une couche d'abstraction entre les outils et les agents. Chaque agent sait ce dont il a besoin et va chercher ses donn√©es directement via Pipedream Connect. C'est exactement ce que fait Lindy. Simple, rapide, √©prouv√©.

**Couche infra : Pipedream Connect**

Pipedream g√®re toute la plomberie : OAuth, tokens, refresh, rate limits, retries, et fournit des actions pr√©-built pour 2,800+ APIs. C'est ce qu'utilise Lindy pour ses "4,000+ int√©grations."

**Co√ªt :** $150/mois + $2/user/mois. √Ä ‚Ç¨160/mois minimum c√¥t√© client = ~1.2% du revenue. √Ä 1,000 users = $2,150/mois ‚Äî absorb√© dans la marge.

**Comment chaque agent fonctionne :**

```
User connecte ses outils (HubSpot, Gmail, Zendesk, Asana, Stripe, BambooHR...)
        ‚Üì
[Pipedream Connect] ‚Äî OAuth, tokens, refresh, rate limits, CRUD
        ‚Üì
[Agent] ‚Äî fetch les donn√©es dont IL a besoin, g√©n√®re un output, eval L1/L2/L3
        ‚Üì
Output ‚Üí Approval Queue ‚Üí User envoie/approuve ou modifie ‚Üí Style Learner capture
```

Pas de couche interm√©diaire. Pas d'abstraction. L'agent est autonome.

**Exemple concret ‚Äî Deal Revival Agent (Sales) :**

1. Fetch CRM (via Pipedream) : deals sans activit√© > 30 jours ‚Üí trouve Acme Corp, ‚Ç¨23K, stale depuis 34j
2. Fetch Email (via Pipedream) : derniers emails avec le contact ‚Üí Pierre a dit de revenir le 20 jan
3. Fetch Calendar (via Pipedream) : prochain meeting ? ‚Üí aucun planifi√©
4. Draft : "Bonjour Pierre, je souhaitais revenir vers vous suite √† notre √©change du 15 janvier sur le pricing en 3 phases..."
5. Eval L1 : ‚úÖ nom correct, ‚úÖ r√©f√©rence √©change r√©el. L2 : ‚úÖ ton coh√©rent. Score : 87
6. ‚Üí Approval Queue

**Exemple concret ‚Äî Urgent Ticket Alert Agent (Support) :**

1. Fetch Zendesk (via Pipedream) : tickets ouverts + `sla_policy` ‚Üí trouve Ticket #1204, client Premium, SLA first reply dans 2h, non assign√©
2. Fetch CRM (via Pipedream) : le contact est client ‚Ç¨31K/an, 3 tickets ouverts en parall√®le
3. Alert : "üî¥ Ticket #1204 (client Premium, ‚Ç¨31K/an) ‚Äî SLA first reply dans 2h, non assign√©. 3 tickets ouverts en parall√®le."
4. Draft r√©ponse : bas√© sur la FAQ interne + historique tickets du client
5. Eval L1 : ‚úÖ num√©ro ticket, ‚úÖ pas de promesse hors scope. L2 : score 92
6. ‚Üí Notification Slack + Draft dans Approval Queue

**Exemple concret ‚Äî Resume Screening Agent (HR) :**

1. Fetch ATS (via Pipedream) : nouvelles candidatures sur "Senior Dev Frontend" ‚Üí 14 CV re√ßus dans les derni√®res 24h
2. Fetch job description : crit√®res obligatoires (React, 3+ ans, FR ou EN)
3. Screening : chaque CV √©valu√© contre les crit√®res. 6 matchent, 5 partiels, 3 hors scope
4. Output : tableau de screening avec score + justification par candidat
5. Eval L1 : ‚úÖ tous les crit√®res √©valu√©s, ‚úÖ pas de biais d√©tect√© (pas de filtre sur √¢ge/genre/origine). L2 : score 88
6. ‚Üí Approval Queue (le DRH review avant de passer en entretien)

**Exemple concret ‚Äî Campaign Health Monitor (Marketing) :**

1. Fetch Mailchimp (via Pipedream) : derni√®res campagnes email + rapports ‚Üí open rate campagne "Webinar F√©v" = 12%
2. Fetch historique : moyenne des 10 derni√®res campagnes = 28%
3. Alert : "üü° Campagne 'Webinar F√©v' : open rate 12% (vs. 28% moyenne). Chute de 57%."
4. Draft : suggestions d'am√©lioration (sujet, timing, segment)
5. Eval L1 : ‚úÖ pourcentages v√©rifi√©s, ‚úÖ campagne identifi√©e. L2 : score 85
6. ‚Üí Notification Slack + Dashboard Marketing

**Exemple concret ‚Äî Invoice Follow-Up Agent (Finance) :**

1. Fetch Stripe (via Pipedream) : invoices `status = open` + `due_date` d√©pass√© ‚Üí 3 factures overdue, total ‚Ç¨12K
2. Fetch CRM (via Pipedream) : identifier le contact associ√© √† chaque facture
3. Draft relance paiement personnalis√© par facture avec montant, date d'√©ch√©ance, lien de paiement
4. Eval L1 : ‚úÖ montant correct, ‚úÖ nom du contact, ‚úÖ lien de paiement inclus. L2 : score 92
5. ‚Üí Approval Queue (le comptable valide avant envoi)

Chaque agent peut croiser plusieurs sources SI c'est pertinent pour son job. Pas parce qu'une architecture l'impose, mais parce que l'agent en a besoin. Le Deal Revival Agent croise CRM + Email + Calendar naturellement. L'Urgent Ticket Alert croise Zendesk + CRM. Le Resume Screening Agent croise l'ATS + job description. Le Campaign Health Monitor ne lit que Mailchimp ‚Äî pas besoin de plus. L'Invoice Follow-Up croise Stripe + CRM pour personnaliser la relance.

**Pourquoi pas les alternatives :**
- Unified APIs (Merge, $650/mois) : plus petit d√©nominateur commun, pas de custom fields
- Nango (open source) : 500 APIs vs 2,800, self-hosting co√ªte du temps de dev
- Build from scratch : Lindy a d√©pens√© $1M pour 250 int√©grations en 2 ans

**Risque Workday :** Pipedream rachet√© par Workday. Si direction change dans 18+ mois, on aura valid√© le PMF et on pourra internaliser les 5-6 connecteurs critiques + Nango.

**Cat√©gories d'outils support√©s par m√©tier :**

| Cat√©gorie m√©tier | Outils via Pipedream | Agents concern√©s |
|-----------------|---------------------|-----------------|
| **Productivit√© (socle)** | Gmail, Outlook, Google Calendar, Outlook Calendar, Slack, Teams, Discord, Google Drive, Notion, SharePoint | Tous les agents ‚Äî email, calendar et messaging sont le socle |
| **Sales** | HubSpot, Salesforce, Pipedrive, Zoho, Close, Copper, LinkedIn Sales Nav, Apollo, Clearbit, DocuSign, PandaDoc | Deal Revival, Follow-Up, Lead Scorer, Pipeline Reporter, Deep Enrichment, Cold Email Sequencer |
| **Marketing** | Mailchimp, ActiveCampaign, Brevo, HubSpot Marketing, Klaviyo, Google Ads, Meta Ads | Campaign Health Monitor, Content Repurposing, Newsletter Writer, SEO Blog Writer, Brand Monitor |
| **Ops** | Asana, Monday, Trello, ClickUp, Zendesk, Intercom, Freshdesk, Typeform, Google Forms | AI Todos Manager, Overdue Task Nudger, Project Status Updater, Client Health Monitor, Support FAQ Generator, Action Item Tracker |
| **HR** | BambooHR, Personio, Gusto, Greenhouse, Lever | Onboarding Checklist, Candidate Follow-Up, Document Collector |
| **Finance** | Stripe, QuickBooks, Xero, Pennylane | Invoice Follow-Up, Vendor Invoice Tracker, Subscription Churn Alert |

**Ce qu'on ne construit PAS :**
- Pas de "Context Builder" comme abstraction s√©par√©e ‚Äî chaque agent fetch ce dont il a besoin
- Pas de "Crossing Engine" ‚Äî le croisement est dans la logique de chaque agent, pas dans une couche
- Pas d'int√©grations e-commerce (Shopify) au launch ‚Äî on se focus sur les outils transversaux des PME

**L'avantage de cette approche :**
- **Simplicit√© :** chaque agent est autonome, facile √† d√©bugger, facile √† am√©liorer
- **Vitesse de dev :** avec Claude Code, chaque agent template = quelques heures de dev, pas des jours
- **Scalabilit√© :** ajouter un agent = √©crire sa logique de fetch + son prompt, pas rewirer une architecture
- **93 templates en 3 semaines** = faisable parce que chaque template suit le m√™me pattern (trigger + fetch donn√©es + prompt + eval + action)

**Comparaison avec Lindy :**

| | Lindy | Nous |
|---|---|---|
| **Int√©grations** | 4,000+ via Pipedream | 2,800+ via Pipedream (m√™me infra) |
| **Agents** | 1,000+ templates | ~93 templates au launch, extensible |
| **Approche** | Agent autonome, trigger ‚Üí action | Identique + scan ‚Ç¨ et style learner |
| **Onboarding** | Configure ton workflow | Plug-and-play : connecte tes outils, les agents travaillent |
| **Ce qu'on a en plus** | ‚Äî | Scan transversal (d√©tection proactive sur sales, support, marketing, HR, finance, projets), Style Learner (les drafts deviennent les tiens) |
| **Ce qu'on a en moins** | ‚Äî | Moins de templates (100 vs 1,000), moins de maturit√©, pas de track record |

**Le wedge se trouvera en usage.** Au launch, on est un Lindy avec scan ‚Ç¨ et style learner. En M1-M3, les donn√©es d'usage nous diront quel segment, quel agent, quelle feature fait la diff√©rence. Le vrai wedge √©merge, on ne le d√©cr√®te pas.


### 2.8 Data Collection Layer

On ne construit pas de moteur pr√©dictif. On collecte.

| Donn√©e | Source | Usage imm√©diat | Usage futur |
|--------|--------|----------------|-------------|
| **Action Log** | Chaque action agent + r√©sultat | Score L2, m√©triques | Am√©lioration prompts |
| **Draft Diffs** | Draft original vs version envoy√©e | Style Learner | Fine-tuning par compte |
| **Interaction Patterns** | Metadata (qui‚Üîqui, fr√©quence, temps r√©ponse, tickets, t√¢ches) | Scan tendances, alertes | D√©tection patterns par vertical |
| **Agent Performance** | Taux d'approbation, modification, rejet par agent | Dashboard, priorit√©s dev | Recommandation agents |

Principe : m√©tadonn√©es, diffs, patterns. JAMAIS le contenu brut des emails ou tickets.

---

## 3. Dogfooding ‚Äî Le pilier strat√©gique

Ce n'est pas une tactique marketing. C'est la strat√©gie produit.

### 3.1 Pourquoi c'est central

Guillaume prospectait pour Lemlist AVEC Lemlist. Quand un prospect recevait un cold email avec son nom sur un whiteboard et bookait un call, Guillaume pouvait dire : "Tu viens de vivre le produit."

Nous : on g√®re notre bo√Æte enti√®rement avec nos propres agents.

| Op√©ration | Agent utilis√© | Preuve produit |
|-----------|--------------|----------------|
| **Sales** ‚Äî Prospection outbound | Cold Email Sequencer + Lead Research + LinkedIn Outreach | "Ce cold email a √©t√© √©crit par notre agent" |
| **Sales** ‚Äî Follow-up prospects | Deal Revival + Follow-Up Agent | "Notre agent a d√©tect√© que tu n'avais pas r√©pondu en 9 jours" |
| **Sales** ‚Äî R√©ponse leads entrants | Speed-to-Lead | "Notre agent t'a r√©pondu en 3 minutes" |
| **Support** ‚Äî R√©ponses utilisateurs | Customer Support Email Responder + FAQ Builder | "La r√©ponse support que tu as re√ßue a √©t√© draft√© par notre agent" |
| **Support** ‚Äî Triage tickets | Email Triager + Urgent Ticket Alert | "Les tickets critiques sont escalad√©s en <5 min" |
| **Marketing** ‚Äî Contenu | Content Repurposing + Newsletter Writer + SEO Blog Writer | "Cet article de blog et cette newsletter ont √©t√© g√©n√©r√©s par nos agents" |
| **Marketing** ‚Äî Veille | Brand Monitor + Competition Tracker | "On sait quand quelqu'un parle de nous ou de nos concurrents en temps r√©el" |
| **HR** ‚Äî Recrutement | Resume Screening + Candidate Follow-Up | "Le screening de nos candidats est fait en 2 min au lieu de 3 jours" |
| **Ops** ‚Äî Meetings & projets | Meeting Prep + Action Item Tracker + Project Status Updater | "Le brief de ce call a √©t√© pr√©par√© par notre agent" |
| **Ops** ‚Äî CRM propre | CRM Data Guardian + Deep Enrichment | "Notre HubSpot est nettoy√© automatiquement chaque jour" |
| **Ops** ‚Äî Reporting | Pipeline Reporter + Daily Ops Digest | "Nos reports hebdo sont g√©n√©r√©s automatiquement" |

### 3.2 Le cercle vertueux

```
Martin utilise les agents pour g√©rer la bo√Æte
  ‚Üí Trouve des bugs/frictions ‚Üí Am√©liore le produit
  ‚Üí Documente les r√©sultats ‚Üí Contenu authentique
  ‚Üí La communaut√© voit les r√©sultats ‚Üí Veut le m√™me outil
  ‚Üí Deviennent users ‚Üí Leur feedback am√©liore le produit
  ‚Üí Martin documente les am√©liorations ‚Üí Plus de contenu
  ‚Üí etc.
```

C'est le "Growth Circle of Love" de Guillaume, appliqu√© √† notre produit.

### 3.3 Chaque agent est dogfood√© avant d'√™tre lanc√©

Pas de templates th√©oriques. Chaque agent dans le catalogue est un agent que Martin ou Ombeline utilise, avec de vrais r√©sultats document√©s.

"Le Deal Revival Agent a √©t√© test√© sur notre propre pipeline pendant 3 semaines. R√©sultat : 12 deals r√©activ√©s, ‚Ç¨34K de pipeline remis en mouvement, taux de modification des drafts pass√© de 68% √† 22%."

"Le Customer Support Email Responder tourne sur nos propres tickets depuis 2 semaines. R√©sultat : temps de premi√®re r√©ponse pass√© de 4h √† 45 min. 60% des r√©ponses envoy√©es sans modification."

"Le Content Repurposing Agent transforme chaque article de blog en 5 formats (LinkedIn post, tweet thread, newsletter snippet, r√©sum√© Slack, short video script). On publie 5x plus avec le m√™me effort."

Personne d'autre ne peut dire √ßa. Les templates de Lindy sont des configurations g√©n√©riques. Les n√¥tres sont battle-tested sur une vraie bo√Æte, avec des screenshots r√©els et des m√©triques publiques.

---

## 4. La communaut√© ‚Äî Le craft, pas le produit

### 4.1 Le mod√®le Lemlist

Guillaume n'a pas cr√©√© "The Lemlist Users Group." Il a cr√©√© **"The Sales Automation Family"** ‚Äî une communaut√© sur le M√âTIER de la prospection. Les gens venaient pour apprendre √† mieux prospecter. Lemlist √©tait le tool montr√© dans chaque exemple.

Contenu type dans la communaut√© Lemlist :
- Templates de campagnes r√©elles avec taux de r√©ponse
- "Lemlister of the Week" : un user showcas√© avec sa campagne, ses r√©sultats
- Discussions sur les meilleures techniques de cold email
- Guillaume qui partage ses propres campagnes

### 4.2 Notre communaut√©

**Nom de travail : "AI Ops Community"** (ou meilleur) ‚Äî une communaut√© sur le craft de l'automatisation business avec l'AI.

Pas "comment utiliser notre produit." Mais "comment utiliser l'AI pour g√©rer ta bo√Æte."

**Contenu type :**

- "Comment j'ai relanc√© 15 deals dormants en 20 minutes ‚Äî voici les drafts (avant/apr√®s)"
- "Mon agent support a r√©solu 40% des tickets sans intervention humaine ‚Äî setup complet"
- "Before/after : le draft ChatGPT vs le draft avec contexte complet (email, ticket, candidature)"
- "Le scan de mon entreprise a trouv√© 3 deals dormants, 5 tickets proches SLA, 6 CV en attente, et ‚Ç¨12K de factures overdue."
- "Semaine 4 avec le Style Learner : mes drafts sont modifi√©s √† 25% contre 70% la premi√®re semaine"
- "Comment mon agent RH a screen√© 200 CV en 30 minutes ‚Äî crit√®res, r√©sultats, surprises"
- "Mon agent marketing recycle chaque article en 5 formats ‚Äî voici le workflow complet"
- "Comment le scan a d√©tect√© ‚Ç¨15K de factures overdue que personne ne suivait ‚Äî 3 relances envoy√©es en 10 min"

**"Agent of the Week" :** Chaque semaine, un user showcas√© avec ses agents, ses r√©sultats r√©els, son setup. L'user partage avec son r√©seau (visibilit√© gratuite). Les autres veulent √™tre le prochain.

### 4.3 Build in public

Martin et Ombeline documentent TOUT. Revenue, erreurs, d√©cisions produit, r√©sultats des agents sur leur propre bo√Æte. Transparence totale.

"Day 3 : j'ai lanc√© le scan sur nos outils. R√©sultat : 3 deals dormants (‚Ç¨47K), 5 tickets support proches du SLA, 2 MQLs non transmis, 4 CV non trait√©s depuis 3 jours, 2 factures overdue, 8 t√¢ches sans assign√©. Embarrassant mais r√©v√©lateur. Les actions sont pr√™tes."

"Week 2 : le Style Learner commence √† √©crire comme moi. Le draft de relance pour Client X mentionnait le pricing en 3 phases qu'on avait discut√© ‚Äî je n'aurais pas pens√© √† le mentionner moi-m√™me. C√¥t√© support : les r√©ponses commencent √† avoir notre ton. C√¥t√© HR : le screening de CV flag les bons crit√®res sans qu'on les re-pr√©cise √† chaque fois."

"Month 1 : 47 actions trait√©es via les agents. Sales : 12 deals relanc√©s, ‚Ç¨34K r√©activ√©. Support : temps de premi√®re r√©ponse pass√© de 4h √† 45 min. Marketing : 8 articles recycl√©s en 40 posts LinkedIn. HR : 45 CV screen√©s en 30 min au lieu de 3 jours. Finance : ‚Ç¨8K de factures overdue relanc√©es automatiquement. Avant les agents : on faisait tout √ßa √† la main."

Chaque post est simultan√©ment : contenu authentique + preuve produit + template r√©plicable.

---

## 5. Implementation

### 5.1 MVP Sprint Plan (3 semaines)

**Semaine 1 : Core Engine + Pipedream + Scan transversal + Agents Sales (first vertical)**

Martin :
- Days 1-2 : Monorepo setup (pnpm + Turbo + packages). Agent execution engine (BullMQ + Redis) + Eval layer (L1/L2/L3) + Pipedream Connect setup + LLM routing (Haiku/Sonnet/Opus via @anthropic-ai/sdk). Credential encryption (AES-256). Config typ√©e (@Env + Zod). Lifecycle hooks + AI event logging. Soumission Google OAuth. Le moteur est g√©n√©rique ‚Äî chaque agent est une config (trigger + donn√©es √† fetcher via Pipedream + prompt + llm_tier + maxStepsPerRun + eval rules). Avec Claude Code, chaque nouveau template = quelques heures.
- Day 3 : Business Scan ‚Äî OAuth pour les outils connect√©s. D√©tection transversale : deals dormants (CRM), tickets proches SLA (support), MQLs non transmis (marketing), candidatures non trait√©es (HR), factures overdue (finance), t√¢ches overdue (projets). Output fram√© par m√©tier. Style Learner core.
- Day 4 : Deal Revival Agent + Follow-Up Agent + CRM Data Guardian. Connecteurs HubSpot/Pipedrive/Salesforce via Pipedream.
- Day 5 : Deep Enrichment + Lead Scorer + Speed-to-Lead.
- Days 6-7 : Pipeline Reporter + Deal Alert. Outbound Engine + Cold Email Sequencer + Lead Research.

Ombeline :
- Jour 1 : Domaines + DNS + mailboxes + privacy policy + Google verification video
- Jour 2-14 : Warmup + listes prospects + s√©quences cold email (dogfooding d√®s que les agents outbound sont pr√™ts)

**Semaine 2 : Agents Support + Ops + HR + Marketing + Templates en masse**

- Day 8 : Support agents core ‚Äî Customer Support Email Responder + Email Triager + Urgent Ticket Alert + Support Ticket Dispatcher. Connecteurs Zendesk/Freshdesk/Intercom via Pipedream.
- Day 9 : Client Health Monitor + Meeting Prep/Recap + Action Item Tracker
- Day 10 : Marketing agents ‚Äî Content Repurposing + Newsletter Writer + SEO Blog Writer + Brand Monitor
- Day 11 : LinkedIn Outreach + Outbound Reporter + Cold Email polish
- Day 12 : HR agents (Resume Screening, Candidate Follow-Up, Employee Onboarding Assistant)
- Day 13 : Daily Briefing par persona (agr√©gation par m√©tier : sales, support, ops, marketing). Ops agents : Overdue Task Nudger + Vendor Invoice Tracker.
- Day 14 : Martin et Ombeline dogfoodent le produit sur leur propre bo√Æte. Bug bash. UX fixes.

**Semaine 3 : Workflow Editor + Templates suppl√©mentaires + Polish + Launch**

- Day 15 : React Flow workflow editor ‚Äî affichage des templates en nodes visuels, customisation par l'user (modifier triggers, conditions, actions). Chaque template = un workflow pr√©-configur√© √©ditable.
- Day 16 : ~60-70 templates suppl√©mentaires via Claude Code (les ~20-30 core sont faits en semaine 1-2, reste ~60-70 sur les 93). Chaque template = trigger + fetch Pipedream + prompt + llm_tier + eval rules.
- Day 17 : Onboarding wizard (connecter outils ‚Üí scan transversal ‚Üí r√©sultats par m√©tier ‚Üí activation agents). Approval Queue. Agent Dashboard.
- Day 18 : Catalogue UI (browse par cat√©gorie : sales, marketing, ops, HR, productivit√©). Credit tracking + Stripe. Error handling.
- Day 19 : Communaut√© setup. Premiers posts build in public.
- Days 20-21 : Testing, edge cases, deployment, soft launch aux 20-50 premiers users.

**Pourquoi ~93 templates en 3 semaines est faisable :** Le moteur d'ex√©cution est g√©n√©rique. Chaque agent est une configuration : trigger + donn√©es √† fetcher + prompt + eval rules. Pipedream g√®re toute la plomberie (2,800+ APIs). Avec Claude Code, un template complet se cr√©e en 2-4 heures. Les 20-30 templates les plus critiques (semaines 1-2) sont les plus complexes et les plus dogfood√©s. Les templates restants (semaine 3) suivent le m√™me pattern et sont g√©n√©r√©s rapidement via Claude Code.

### 5.2 Ce qu'on NE construit PAS en V1

| Feature | Pourquoi pas maintenant | Quand |
|---------|------------------------|-------|
| Chat Builder (agents custom from scratch) | Trop ambitieux. ~93 templates customisables via React Flow couvrent la majorit√© des besoins. | V2 quand on comprend les besoins custom via le feedback |
| Impact Engine / Pulse Report | Pas assez de data Day 1 pour des m√©triques cr√©dibles | V1.1 (M2-M3) |
| Marketplace templates | Pas assez d'users pour un √©cosyst√®me | V3+ |
| Enterprise tier (SSO/SAML) | Pas la cible Day 1 | V4+ |

### 5.3 Risques techniques critiques

| Risque | S√©v√©rit√© | Mitigation | Source insight |
|--------|----------|------------|---------------|
| OAuth "Unverified App" warnings (Gmail, Zendesk, etc.) | üî¥ | Soumettre v√©rification semaine 1 pour chaque provider. Fallback : 100 users test. Prioriser les outils les plus demand√©s. | ‚Äî |
| Qualit√© des drafts | üî¥ | Tout en draft-only. Eval L1/L2/L3 obligatoire. Style Learner. Dogfooding intensif. `maxStepsPerRun: 5` pour limiter les co√ªts. | Dust : maxStepsPerRun = 3 |
| Credential security (on stocke des tokens OAuth, API keys) | üî¥ | AES-256 encryption de TOUS les credentials. Redaction c√¥t√© frontend (jamais de secret en clair dans le browser). Rotation de l'encryption key document√©e. | n8n : `N8N_ENCRYPTION_KEY` + `redact()` |
| 93 templates en 3 semaines = qualit√© in√©gale | üü° | Chaque agent = m√™me pattern (trigger + fetch Pipedream + prompt + eval). Claude Code acc√©l√®re massivement. Prioriser les 20-30 agents dogfood√©s. Les autres fonctionnent mais sont affin√©s post-launch via feedback. | ‚Äî |
| Privacy donn√©es (emails, tickets, CV, factures) | üü° | Metadata-first. Zero storage contenu brut. Messaging clair par type de donn√©es. Compliance RGPD d√®s Day 1. | ‚Äî |
| Agent execution coup√©e mid-run (deploy, crash) | üü° | Graceful shutdown avec `SHUTDOWN_TIMEOUT=30s`. BullMQ stall detection + retry automatique. Aucun deploy ne kill une ex√©cution en cours. | n8n : graceful shutdown 30s |
| Explosion de co√ªts LLM | üü° | `maxStepsPerRun` par template. AI event logging de CHAQUE appel (model, tokens, cost, latency). Dashboard co√ªt par agent, par user, par mois. Alertes si co√ªt moyen >‚Ç¨0.10/action. | n8n : `logAiEvent()` |
| Pipedream down / API rate limited | üü° | Retry avec backoff exponentiel via BullMQ. Cache des derni√®res donn√©es. Notification user si reconnexion n√©cessaire. | n8n : Bull stall detection |
| Config bugs en production (mauvaise env var) | üü° | Configuration valid√©e par Zod au boot. App refuse de d√©marrer si config invalide. Support `*_FILE` pour secrets Docker. | n8n : `@Env()` + Zod |
| Trop similaire √† Lindy, pas de diff√©renciation | üü° | Le scan transversal et le style learner diff√©rencient au launch. Le vrai wedge se pr√©cise en M1-M3 via usage. Si aucun wedge n'√©merge, pivoter le positionnement. | ‚Äî |
| Scan faux positifs (deals "dormants" qui sont sains) | üü° | Snooze/dismiss par signal. Thresholds configurables par entreprise. Wording "√† v√©rifier" pas "en danger." | Stress-test V5 |
| Debug impossible sur les erreurs agents | üü° | Error type hierarchy d√®s Day 1 : `ScanError`, `AgentExecutionError`, `ConnectorError`, `CredentialError`. Chaque erreur porte le contexte complet. | n8n : NodeOperationError |

### 5.4 User Journey: Day 0 ‚Üí Month 2

**Minute 0 :** "Scannez vos outils gratuitement" ‚Üí OAuth sur les outils connect√©s (CRM, email, support, projets...).

**Minute 2 :** Le scan lit les outils connect√©s. Feed live par m√©tier :

Thomas (Sales) voit : "3 deals sans activit√© depuis 7+ jours. 1 lead non trait√©. Drafts pr√™ts." Il clique sur Deal Acme Corp, voit le draft qui mentionne le pricing en 3 phases discut√© le 15 janvier. Il modifie l√©g√®rement, envoie. Style Learner capture.

Julien (Support) voit : "5 tickets proches du SLA. 1 ticket premium non assign√©. 3 tickets ouverts >24h." Il assigne le ticket premium, envoie le draft de r√©ponse.

**Minute 5 :** Thomas active Deal Revival + Follow-Up + Speed-to-Lead. Julien active Urgent Ticket Alert + Support FAQ Generator + Customer Sentiment Tracker.

**Day 1 :** Chaque persona re√ßoit son daily briefing adapt√©. Thomas : 2 deals √† relancer, 1 lead entrant. Julien : 1 ticket premium proche SLA, temps de r√©solution moyen en hausse. En 15 min, les urgences sont trait√©es. Ils browsent le catalogue pour activer d'autres agents.

**Day 3 :** Thomas invite Sophie (Head of Ops, +‚Ç¨15/seat). Sophie connecte Asana + Stripe. Le scan trouve : 8 t√¢ches overdue, 3 sans assign√©, ‚Ç¨12K de factures impay√©es. Elle active Overdue Task Nudger + Vendor Invoice Tracker + Meeting Prep.

**Week 1 :** L'√©quipe a trait√© 30 outputs d'agents. 20 envoy√©s sans modification, 10 modifi√©s. Le Style Learner apprend les patterns de chacun ‚Äî Thomas √©crit court et direct, Julien est plus empathique dans ses r√©ponses support, Sophie est tr√®s structur√©e.

**Week 2 :** Le Deal Acme Corp a r√©pondu. Meeting replanifi√©. C√¥t√© support, le temps de premi√®re r√©ponse est pass√© de 4h √† 45 min gr√¢ce aux drafts automatiques. CRM Data Guardian a trouv√© 23 doublons dans HubSpot.

**Week 3 :** Taux de modification global : 40% (vs 70% semaine 1). Thomas a 8 agents actifs, Julien 6, Sophie 5. Cr√©dits en approche ‚Üí Starter.

**Month 1 :** Sales : ‚Ç¨23K de pipeline r√©activ√©. Support : SLA breach rate √∑2. Ops : 0 t√¢ches overdue non trait√©es. Finance : ‚Ç¨8K de factures relanc√©es. 3 personas actifs, chacun avec des agents adapt√©s √† son m√©tier.

**Month 2 :** Claire (Marketing) rejoint (+‚Ç¨15/seat). Active Content Repurposing + Newsletter Writer + SEO Blog Writer. Antoine (HR) rejoint (+‚Ç¨15/seat). Active Resume Screening + Candidate Follow-Up + Employee Onboarding Assistant. L'√©quipe demande un agent de reporting consolid√© cross-m√©tier ‚Üí signal pour le construire.

### 5.5 Stack Technique ‚Äî Fond√© sur le reverse-engineering de Dust & n8n

> **M√©thodologie :** Chaque choix technique ci-dessous est justifi√© par l'analyse des repos GitHub de Dust.tt ($7.3M ARR, 84 contributeurs, 17,842 commits) et n8n (162K stars, 400+ int√©grations). On a identifi√© ce que les deux convergent (PostgreSQL, Redis, TypeScript), ce qu'ils font diff√©remment, et les anti-patterns √† √©viter.

**Framework & Runtime**

| Composant | Choix | Justification (Dust/n8n insight) |
|-----------|-------|----------------------------------|
| **Node.js 22 LTS** | Runtime | Les deux utilisent Node.js. Dust ajoute Rust pour le chunking haute perf ‚Äî pas n√©cessaire √† notre volume (1000x inf√©rieur). |
| **TypeScript 5.x strict** | Langage | Les deux l'utilisent avec strict mode. n8n : z√©ro `any` dans les packages core. On fait pareil. |
| **Next.js 14+ (App Router)** | Framework | Dust utilise Next.js (Pages Router, deprecated). On prend App Router directement. SSR, API routes, un seul d√©ploiement. |
| **React 19** | UI | Standard. Dust utilise React + leur design system Sparkle. |
| **Prisma 6 + PostgreSQL** | ORM + DB | Dust utilise Sequelize (ancien), n8n utilise TypeORM (lourd). Prisma est sup√©rieur en type-safety et DX. Les deux convergent sur PostgreSQL. |
| **Supabase Auth** | Authentification | Dust utilise Auth0 (enterprise, cher). n8n utilise JWT custom. Supabase Auth = OAuth social + magic link + session management, int√©gr√© √† notre Supabase PostgreSQL. |
| **tRPC + TanStack Query** | API layer | Dust utilise SWR (simple mais limit√©). TanStack Query = cache, mutations, optimistic updates, invalidation. tRPC = type-safety end-to-end. |
| **Tailwind CSS v4 + shadcn/ui** | Styling | Dust a Sparkle (custom), n8n a un design system Vue custom. shadcn/ui + Radix = composants React accessibles, customizables, z√©ro maintenance. |
| **Sentry** | Monitoring | n8n utilise `@sentry/node` + `@sentry/profiling-node`. Error tracking + performance + AI cost tracking. |
| **ESLint + Prettier** | Dev tooling | Les DEUX utilisent ESLint (Dust a m√™me un plugin ESLint custom). L'√©cosyst√®me ESLint est plus riche que Biome. |
| **pnpm + Turborepo** | Monorepo | n8n utilise pnpm + Turbo. Builds parall√®les, caching agressif (<30s avec cache). Yarn workspaces (Dust) est plus ancien. |

**Moteur d'ex√©cution des agents ‚Äî Inspir√© de Bull (n8n) + Temporal (Dust)**

| Composant | Choix | Justification (Dust/n8n insight) |
|-----------|-------|----------------------------------|
| **BullMQ + Redis** | Queue & orchestration | n8n utilise Bull (pr√©d√©cesseur de BullMQ) pour TOUTES ses ex√©cutions : priority queues, concurrency control, retries, stall detection, graceful shutdown. Dust utilise Temporal (10M+ jobs/day) ‚Äî overkill pour nous. BullMQ = Bull moderne, TypeScript natif, m√™me concepts. On contr√¥le notre infra d'ex√©cution. |
| **ioredis** | Client Redis | Le client Redis de n8n. Feature-rich : clustering, sentinel, pipelines. Redis sert pour BullMQ + PubSub (streaming SSE) + cache. |
| **Pipedream Connect** | Int√©grations OAuth | OAuth + tokens + CRUD sur 2,800+ APIs. L'agent appelle Pipedream pour lire Gmail, √©crire dans HubSpot. On ne g√®re AUCUN OAuth soi-m√™me. ~$2/user/mois. |
| **@anthropic-ai/sdk** | LLM direct | n8n utilise LangChain (overhead significatif, debugging opaque). Dust utilise des SDK directs. On suit Dust : Anthropic SDK directement, pas d'abstraction. Moins de couches = moins de bugs. |
| **React Flow (@xyflow/react)** | Workflow editor | L'user part d'un template et customise : ajouter/retirer des √©tapes, modifier les conditions, changer les actions. Identique au mod√®le Lindy. |

**Pourquoi BullMQ au lieu d'Inngest :**

| Crit√®re | Inngest | BullMQ + Redis |
|---------|---------|----------------|
| **Setup** | 1 ligne SDK, managed | Redis √† provisionner (Upstash = 1 clic) |
| **Co√ªt** | $0 pour <25K runs, puis $25+/mois | Redis : $0-10/mois (Upstash free tier) |
| **Contr√¥le** | Bo√Æte noire, vendor lock-in | Full control, open-source |
| **Debugging** | Dashboard Inngest | BullMQ Dashboard + nos propres logs |
| **Graceful shutdown** | Non garanti | Natif (n8n le fait en production) |
| **Stall detection** | N/A | Natif (n8n l'utilise pour les ex√©cutions zombies) |
| **Priority queues** | Non | Oui (scans urgents > scans planifi√©s) |
| **Redis d√©j√† n√©cessaire** | Non | Oui (SSE streaming + cache + rate limiting) |

**Verdict :** On a D√âJ√Ä besoin de Redis pour le streaming SSE (pattern Dust), le cache, et le rate limiting. BullMQ est un ajout marginal sur un Redis existant. Pas de vendor lock-in, contr√¥le total, et les patterns sont prouv√©s par n8n √† 162K stars.

**Pourquoi Anthropic SDK direct au lieu de Vercel AI SDK :**

| Crit√®re | Vercel AI SDK | @anthropic-ai/sdk |
|---------|---------------|-------------------|
| **Abstraction** | Multi-provider (OpenAI, Anthropic, Google, etc.) | Claude uniquement |
| **Streaming** | ‚úÖ Excellent | ‚úÖ Natif |
| **Tool use** | ‚úÖ Via adapter | ‚úÖ Natif, type-safe |
| **Debugging** | 1 couche d'abstraction en plus | Direct ‚Äî les erreurs sont celles de l'API |
| **Model switching** | 1 ligne de code | Notre wrapper `@nodebase/ai/tiering.ts` (10 lignes) |

**Verdict :** On utilise Claude exclusivement Day 1. L'abstraction Vercel AI SDK ne vaut pas le co√ªt en debugging. Si on ajoute GPT-4o ou Gemini en Month 2-3, un thin wrapper de 50 lignes suffit. Dust fait exactement √ßa ‚Äî SDK directs, pas de LangChain.

**Routing LLM par complexit√©**

| Tier | Mod√®le | Co√ªt input/1M tokens | Cas d'usage | % du volume |
|------|--------|---------------------|-------------|-------------|
| **Fast** | Claude Haiku 3.5 | $0.80 | Triage, extraction donn√©es structur√©es, r√©sum√©s, notifications, alertes | ~60% |
| **Smart** | Claude Sonnet 4 | $3.00 | Follow-ups, r√©ponses tickets, meeting recaps, screening CV, templates | ~30% |
| **Deep** | Claude Opus 4.5 | $15.00 | Drafts strat√©giques, analyse sentiment complexe, style learner training | ~10% |

**Le routing est simple :** chaque template d'agent sp√©cifie son tier dans sa config. Pas de routing dynamique Day 1 ‚Äî c'est un champ `llm_tier: "fast" | "smart" | "deep"` dans la config. Copi√© du pattern Dust `SUPPORTED_MODEL_CONFIGS`.

**Impact co√ªts :** Co√ªt moyen pond√©r√© ‚âà $1.50/1M tokens. Sur 1,000 users actifs avec ~500K tokens/user/mois = ~$750/mois de LLM. Marge saine.

```typescript
// Agent template ‚Äî m√™me pattern pour TOUS les 93 agents
const followUpAgent: AgentTemplate = {
  id: "follow-up-simple",
  name: "Follow-Up Agent",
  trigger: { type: "cron", schedule: "0 9 * * *" },
  fetch: [
    { source: "gmail", query: "is:sent after:3d no:reply" },
    { source: "hubspot", query: "deals.where(stage != 'closed')" }
  ],
  llm_tier: "smart",                    // ‚Üí Claude Sonnet
  maxStepsPerRun: 5,                    // guard-rail co√ªt (Dust met 3)
  prompt: "Tu es un assistant commercial. Voici les emails sans r√©ponse...",
  eval_rules: {
    assertions: [
      { check: "contains_recipient_name", severity: "block" },
      { check: "no_placeholders", severity: "block" },
      { check: "references_real_exchange", severity: "block" },
      { check: "correct_language", severity: "warn" }
    ],
    min_confidence: 0.6,
    l3_trigger: "on_irreversible_action",
    require_approval: true,
    auto_send_threshold: 0.85
  },
  actions: [
    { type: "draft_email", require_approval: true },
    { type: "update_crm", field: "last_followup_date" }
  ]
}
```

**Flow d'ex√©cution complet (BullMQ + Redis PubSub) :**

```
[BullMQ] trigger (cron job / webhook / event)
    ‚Üì
[Agent Config] ‚Üí quelles donn√©es fetcher ? quel prompt ? quel LLM tier ? maxStepsPerRun ?
    ‚Üì
[Pipedream Connect] ‚Üí fetch les sources (CRM, email, support, ATS, projets, calendar...)
    ‚Üì
[@anthropic-ai/sdk] ‚Üí route vers le bon mod√®le (Haiku/Sonnet/Opus)
                    ‚Üí prompt = template + donn√©es fetch√©es
                    ‚Üí streaming via Redis PubSub ‚Üí SSE vers le frontend
    ‚Üì
[Lifecycle Hooks] ‚Üí agent.before / agent.after (logging, cost tracking, notifications)
    ‚Üì
[Eval Layer] ‚Üí L1 assertions (deterministic, <10ms) ‚Üí si fail block ‚Üí r√©g√©n√®re
           ‚Üí L2 confidence score (rule-based, <50ms) ‚Üí score 0-100
           ‚Üí L3 LLM-as-Judge (Haiku, ~200ms, ~$0.00006) ‚Üí si triggered
    ‚Üì
[AI Event Log] ‚Üí model, tokens_in, tokens_out, cost, latency, agent_id, user_id
    ‚Üì
[Prisma + Resource Pattern] ‚Üí output stock√© en DB avec metadata + permissions
    ‚Üì
[Redis PubSub ‚Üí SSE] ‚Üí user voit l'output en streaming dans le dashboard
    ‚Üì
[User] ‚Üí approuve / modifie / rejette
    ‚Üì
[BullMQ] ‚Üí si approuv√© : Pipedream ex√©cute l'action (email send, ticket reply, CRM update...)
[Style Learner] ‚Üí si modifi√© : capture les diffs pour am√©liorer les prochains outputs
[AI Event Log] ‚Üí action_taken, result, feedback stock√©s
```

**Ce qu'on ne g√®re PAS nous-m√™mes :**

| Responsabilit√© | Qui g√®re | Pourquoi |
|---------------|---------|---------|
| OAuth / tokens | Pipedream | 2,800 APIs, refresh tokens, scopes. |
| Job scheduling / retries / stall detection | BullMQ + Redis | Open-source, battle-tested par n8n. |
| Error tracking + performance | Sentry | `@sentry/node` + `@sentry/profiling-node`. |
| Hosting frontend | Vercel | Next.js natif, CDN, serverless. |
| Database managed | Supabase | PostgreSQL managed. Backups automatiques. Auth int√©gr√©. |
| Redis managed | Upstash | Serverless Redis. Free tier ‚Üí $10/mois. |

**State management :**

| Besoin | Outil | Justification |
|--------|-------|--------------|
| State serveur (agents, drafts, configs) | tRPC + TanStack Query | Cache, optimistic updates, invalidation. 90% du state est serveur. |
| State client (React Flow editor) | Zustand | State management l√©ger pour les nodes/edges du workflow editor. Plus simple que Jotai. |
| Params URL | Nuqs | Filtres catalogue, pagination. Type-safe. |

**Formulaires & Validation :**
React Hook Form + Zod. Zod est utilis√© par les DEUX plateformes (Dust et n8n) pour la validation runtime.

---

### 5.6 Architecture ‚Äî Patterns prouv√©s par Dust & n8n

> **Principe :** On ne r√©invente rien. Chaque pattern ci-dessous est extrait du code de Dust ou n8n, adapt√© √† notre contexte.

**Pattern 1 ‚Äî Resource Pattern (source : Dust)**

Dust n'expose JAMAIS un mod√®le Sequelize brut dans une API route. Chaque mod√®le est envelopp√© dans une classe `*Resource` qui v√©rifie les permissions, s√©rialise proprement, et encapsule la business logic.

```typescript
// Au lieu de : const scan = await prisma.scan.findUnique({ where: { id } })
// On fait :
class ScanResource {
  static async findById(scanId: string, auth: Authenticator): Promise<ScanResource | null> {
    const scan = await prisma.scan.findUnique({ where: { id: scanId } });
    if (!scan) return null;
    if (!auth.canAccess(scan.workspaceId)) throw new PermissionError();
    return new ScanResource(scan, auth);
  }

  toJSON() {
    return {
      id: this.scan.id,
      signals: this.scan.signals,
      createdAt: this.scan.createdAt,
      // credentials JAMAIS expos√©s
    };
  }
}
```

Resources √† impl√©menter Day 1 : `ScanResource`, `AgentTemplateResource`, `ConnectorResource`, `WorkspaceResource`, `CredentialResource`.

**Pattern 2 ‚Äî Lifecycle Hooks (source : n8n)**

n8n injecte du comportement √† chaque √©tape d'ex√©cution via des hooks, sans modifier le moteur core. On fait pareil :

```typescript
const hooks: AgentLifecycleHooks = {
  agentExecuteBefore: async (ctx) => {
    ctx.startTime = Date.now();
    await sentry.startTransaction({ name: `agent:${ctx.agentId}` });
  },
  agentExecuteAfter: async (ctx) => {
    const latency = Date.now() - ctx.startTime;
    await logAiEvent({
      agentId: ctx.agentId,
      model: ctx.model,
      tokensIn: ctx.tokensIn,
      tokensOut: ctx.tokensOut,
      cost: calculateCost(ctx.model, ctx.tokensIn, ctx.tokensOut),
      latency,
    });
  },
  onError: async (ctx, error) => {
    sentry.captureException(error, { extra: { agentId: ctx.agentId } });
  }
};
```

**Pattern 3 ‚Äî Credential Encryption + Redaction (source : n8n)**

n8n chiffre TOUS les credentials avec AES et ne les expose JAMAIS au frontend :

```typescript
// Stockage : AES-256 encryption
const encrypted = encrypt(JSON.stringify(credentials), ENCRYPTION_KEY);
await prisma.credential.create({ data: { encryptedData: encrypted } });

// API response : redaction
function redactCredentials(cred: Credential): SafeCredential {
  return {
    id: cred.id,
    name: cred.name,
    type: cred.type,
    // Le frontend voit "‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢" pas le vrai token
    data: Object.fromEntries(
      Object.keys(cred.data).map(k => [k, '‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢'])
    ),
  };
}
```

**Pattern 4 ‚Äî Error Type Hierarchy (source : n8n)**

n8n a des types d'erreurs sp√©cifiques avec contexte complet. Pas de `throw new Error("something went wrong")` :

```typescript
class ScanError extends NodebaseError {
  constructor(public signalId: string, public connectorId: string, message: string) {
    super(`Scan failed on signal ${signalId} via ${connectorId}: ${message}`);
  }
}

class AgentExecutionError extends NodebaseError {
  constructor(public agentId: string, public stepNumber: number, message: string) {
    super(`Agent ${agentId} failed at step ${stepNumber}: ${message}`);
  }
}

class ConnectorError extends NodebaseError {
  constructor(public connectorType: string, public endpoint: string, public statusCode: number) {
    super(`${connectorType} API error ${statusCode} on ${endpoint}`);
  }
}

class CredentialError extends NodebaseError {
  constructor(public credentialId: string, public errorType: 'expired' | 'invalid' | 'revoked') {
    super(`Credential ${credentialId} is ${errorType}`);
  }
}
```

**Pattern 5 ‚Äî Configuration typ√©e @Env() + Zod (source : n8n)**

n8n mappe les env vars aux propri√©t√©s typ√©es avec validation automatique :

```typescript
class ScanConfig {
  @Env('SCAN_INTERVAL_MINUTES')
  interval: number = 60;

  @Env('SCAN_MAX_SIGNALS')
  maxSignals: number = 23;

  @Env('SCAN_METADATA_ONLY')
  metadataOnly: boolean = true;
}

class LLMConfig {
  @Env('ANTHROPIC_API_KEY')
  apiKey: string;  // fail au boot si absent

  @Env('LLM_MAX_STEPS_PER_RUN')
  maxStepsPerRun: number = 5;

  @Env('LLM_COST_ALERT_THRESHOLD')
  costAlertThreshold: number = 0.10;  // ‚Ç¨/action
}
```

L'app refuse de d√©marrer si la config est invalide. Z√©ro bug de config en production.

**Pattern 6 ‚Äî AI Event Logging (source : n8n)**

n8n track CHAQUE appel LLM via `logAiEvent()`. Non-n√©gociable pour nous :

```typescript
interface AiEvent {
  id: string;               // nanoid
  agentId: string;
  userId: string;
  workspaceId: string;
  model: 'haiku' | 'sonnet' | 'opus';
  tokensIn: number;
  tokensOut: number;
  cost: number;             // en ‚Ç¨
  latency: number;          // en ms
  stepsUsed: number;
  maxStepsAllowed: number;
  evalResult: 'pass' | 'block' | 'warn';
  action: 'draft' | 'send' | 'update' | 'notify';
  timestamp: Date;
}
```

Dashboard admin : co√ªt par agent, par user, par mois. Alertes si un template explose son budget.

**Pattern 7 ‚Äî SSE via Redis PubSub (source : Dust)**

Dust stream les r√©ponses agents via Redis PubSub ‚Üí SSE. Pas de WebSocket. Plus simple √† scaler :

```typescript
// Backend : publish sur Redis
const channel = `agent:${executionId}`;
await redis.publish(channel, JSON.stringify({ type: 'token', data: token }));
await redis.publish(channel, JSON.stringify({ type: 'done', data: result }));

// API route SSE : subscribe
app.get('/api/executions/:id/stream', async (req, res) => {
  res.setHeader('Content-Type', 'text/event-stream');
  const sub = redis.duplicate();
  await sub.subscribe(`agent:${req.params.id}`);
  sub.on('message', (_, data) => res.write(`data: ${data}\n\n`));
});
```

**Pattern 8 ‚Äî Graceful Shutdown (source : n8n)**

n8n attend que les ex√©cutions en cours se terminent avant shutdown :

```typescript
process.on('SIGTERM', async () => {
  console.log('Shutdown signal received. Waiting for active executions...');
  await queue.close(30_000); // 30s grace period
  await prisma.$disconnect();
  process.exit(0);
});
```

---

### 5.7 Architecture cible ‚Äî Monorepo

```
nodebase/ (pnpm + Turborepo)
‚îÇ
‚îú‚îÄ‚îÄ apps/
‚îÇ   ‚îî‚îÄ‚îÄ web/                          # Next.js 14+ App Router
‚îÇ       ‚îú‚îÄ‚îÄ app/                      # Routes (dashboard, scan, agents, settings, catalogue)
‚îÇ       ‚îú‚îÄ‚îÄ components/               # shadcn/ui + custom components
‚îÇ       ‚îî‚îÄ‚îÄ lib/                      # Hooks, API client tRPC, utils
‚îÇ
‚îú‚îÄ‚îÄ packages/
‚îÇ   ‚îú‚îÄ‚îÄ @nodebase/types/                 # Interfaces partag√©es front ‚Üî back
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent.ts                  # AgentTemplate, AgentExecution, AgentAction
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scan.ts                   # ScanResult, Signal, SignalCategory
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ connector.ts              # ConnectorConfig, ConnectorStatus
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ credential.ts             # CredentialType, EncryptedCredential
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ errors.ts                 # ScanError, AgentError, ConnectorError, CredentialError
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ @nodebase/db/                    # Prisma schema + generated client + Resource pattern
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prisma/schema.prisma      # Workspace, User, Agent, Scan, Connector, Credential, AiEvent
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ src/resources/            # ScanResource, AgentResource, ConnectorResource, etc.
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ @nodebase/core/                  # Moteurs d'ex√©cution
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scan-engine/              # D√©tection signaux (metadata-only, 23 signaux, 6 cat√©gories)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent-engine/             # Ex√©cution agents (LLM calls, actions, maxStepsPerRun)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ eval/                     # L1 assertions, L2 scoring, L3 LLM-as-Judge
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ hooks/                    # Lifecycle hooks (before/after scan, agent, action)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ @nodebase/connectors/            # Connecteurs (Pipedream + notre intelligence layer)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.ts                   # BaseConnector interface
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hubspot/                  # HubSpot: deals, contacts, activities
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pipedrive/                # Pipedrive: deals, contacts, activities
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ zendesk/                  # Zendesk: tickets, SLAs, satisfaction
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stripe/                   # Stripe: invoices, subscriptions, payments
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gmail/                    # Gmail: emails, threads
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ calendar/                 # Google Calendar: events, availability
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ @nodebase/queue/                 # BullMQ workers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scan-worker.ts            # Scheduled scans, on-demand scans
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent-worker.ts           # Agent execution (async, streaming)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ sync-worker.ts            # Connector data sync (incremental)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ @nodebase/config/                # Configuration typ√©e (@Env() + Zod)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ env.ts                    # @Env() decorator + Zod validation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ index.ts                  # GlobalConfig (database, queue, llm, scan, connectors)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ @nodebase/crypto/                # Credential encryption (AES-256)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ encrypt.ts                # AES-256 encrypt/decrypt
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ redact.ts                 # Redaction pour le frontend
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ @nodebase/ai/                    # LLM integration
‚îÇ       ‚îú‚îÄ‚îÄ client.ts                 # Anthropic SDK wrapper (thin)
‚îÇ       ‚îú‚îÄ‚îÄ events.ts                 # AI event logging (model, tokens, cost, latency)
‚îÇ       ‚îú‚îÄ‚îÄ style-learner.ts          # Style adaptation par user
‚îÇ       ‚îî‚îÄ‚îÄ tiering.ts                # Model selection (Haiku/Sonnet/Opus)
‚îÇ
‚îú‚îÄ‚îÄ templates/                        # ~93 agent templates (JSON + prompt)
‚îÇ   ‚îú‚îÄ‚îÄ sales/                        # deal-revival, follow-up, lead-scorer, etc.
‚îÇ   ‚îú‚îÄ‚îÄ support/                      # ticket-responder, sla-alert, faq-generator, etc.
‚îÇ   ‚îú‚îÄ‚îÄ marketing/                    # content-repurposing, newsletter-writer, etc.
‚îÇ   ‚îú‚îÄ‚îÄ hr/                           # resume-screening, candidate-follow-up, etc.
‚îÇ   ‚îú‚îÄ‚îÄ finance/                      # invoice-follow-up, expense-tracker, etc.
‚îÇ   ‚îî‚îÄ‚îÄ operations/                   # task-nudger, meeting-prep, daily-briefing, etc.
‚îÇ
‚îú‚îÄ‚îÄ turbo.json                        # Build pipeline config
‚îú‚îÄ‚îÄ pnpm-workspace.yaml               # Workspace config
‚îú‚îÄ‚îÄ .github/workflows/                # CI/CD (lint, test, build, deploy)
‚îú‚îÄ‚îÄ docker/                           # Dockerfiles (si n√©cessaire)
‚îî‚îÄ‚îÄ .env.example                      # Template de config
```

---

### 5.8 D√©pendances open-source ‚Äî Shopping list

> **Chaque d√©pendance ci-dessous est utilis√©e en production par Dust et/ou n8n.** Pas de choix th√©oriques.

**Day 1 ‚Äî Core (non-n√©gociable)**

```json
{
  "dependencies": {
    "next": "^14",
    "react": "^19",
    "@anthropic-ai/sdk": "latest",
    "@prisma/client": "^6",
    "bullmq": "latest",
    "ioredis": "latest",
    "zod": "latest",
    "zod-to-json-schema": "latest",
    "jsonrepair": "latest",
    "axios": "latest",
    "lodash": "latest",
    "luxon": "latest",
    "nanoid": "latest",
    "nodemailer": "latest",
    "jsonwebtoken": "latest",
    "helmet": "latest",
    "dotenv": "latest",
    "change-case": "latest",
    "@sentry/node": "latest",
    "@sentry/profiling-node": "latest",
    "@tanstack/react-query": "latest",
    "@trpc/server": "latest",
    "@trpc/client": "latest",
    "@xyflow/react": "latest",
    "zustand": "latest",
    "react-hook-form": "latest",
    "nuqs": "latest"
  },
  "devDependencies": {
    "typescript": "^5",
    "prisma": "^6",
    "eslint": "latest",
    "typescript-eslint": "latest",
    "eslint-config-prettier": "latest",
    "prettier": "latest",
    "jest": "latest",
    "@playwright/test": "latest",
    "turbo": "latest",
    "husky": "latest"
  }
}
```

**Libs sp√©cifiques √† valeur √©lev√©e :**

| Lib | Vient de | Pourquoi indispensable |
|-----|----------|----------------------|
| **jsonrepair** | n8n | Les LLMs g√©n√®rent souvent du JSON cass√©. R√©pare automatiquement. |
| **nanoid** | n8n + Dust | IDs courts URL-safe (`scan_kx7Gh2p`). Meilleur UX que les UUIDs. |
| **luxon** | n8n | Dates/times avec timezones. Successeur de Moment.js. |
| **change-case** | n8n | Normalise `deal_stage` (HubSpot) ‚Üî `dealStage` (Pipedrive). |
| **ioredis** | n8n | Client Redis feature-rich. BullMQ + PubSub + cache sur un seul client. |
| **zod** | les deux | Validation TypeScript-first. Utilis√© partout : API input, config, LLM output parsing. |

**Month 2-3 ‚Äî Scale**

```
bullmq-dashboard (monitoring des jobs)
@testcontainers/postgresql (tests DB r√©alistes)
@testcontainers/redis (tests queue)
@slack/web-api (int√©gration Slack)
googleapis (connecteurs Google)
@rudderstack/rudder-sdk-node (analytics)
storybook (documentation composants)
highlight.js (si preview code dans l'UI)
boring-avatars (avatars g√©n√©r√©s)
react-markdown (rendu markdown)
```

---

### 5.9 Patterns Day 1 vs Month 2-3 vs Month 4+

**Day 1 ‚Äî impl√©menter imm√©diatement :**

| Pattern | Source | Impact |
|---------|--------|--------|
| Resource Pattern (models ‚Üí resources avec permissions) | Dust | S√©curit√© by design |
| BaseConnector interface | Dust | Chaque nouveau connecteur est pr√©visible |
| Lifecycle Hooks (agent.before, agent.after) | n8n | Monitoring/logging injectable |
| Credential encryption AES-256 + redaction | n8n | Les PME nous confient leurs cl√©s API |
| Error Type Hierarchy | n8n | Debug 10x plus rapide |
| Config @Env() + Zod validation | n8n | Z√©ro bug de config en production |
| AI Event Logging (model, tokens, cost, latency) | n8n | Optimisation co√ªts impossible sans √ßa |
| SSE via Redis PubSub | Dust | Streaming simple, scalable |
| maxStepsPerRun (limite d'it√©rations agent) | Dust | Guard-rail co√ªt, configurable par template |
| Graceful Shutdown (30s timeout) | n8n | Aucune ex√©cution coup√©e mid-run |

**Month 2-3 ‚Äî quand on a du volume :**

| Pattern | Source | Trigger |
|---------|--------|---------|
| Queue mode avec workers s√©par√©s | n8n | >100 users actifs |
| Sub-agent composition (max depth 4) | Dust | Agents simples marchent d'abord |
| Declarative connector format (JSON, pas code) | n8n | >6 connecteurs |
| RBAC par Space/Group | Dust | Multi-team dans un workspace |
| Leader election via Redis TTL | n8n | Multi-instance pour HA |

**Month 4+ ‚Äî enterprise :**

| Pattern | Source | Trigger |
|---------|--------|---------|
| Community agent marketplace | n8n | Communaut√© active |
| LDAP/SAML SSO | n8n | Clients enterprise |
| Prometheus metrics | n8n | Monitoring avanc√© |
| Toolset dynamique (agents auto-discover tools) | Dust | 10+ connecteurs par workspace |

---

### 5.10 M√©triques engineering ‚Äî Targets

| M√©trique | Source d'inspiration | Target Day 1 | Target M3 |
|----------|---------------------|-------------|-----------|
| **Build time** (full rebuild) | n8n Turbo caching | < 2min cold | < 30s avec cache |
| **Type coverage** | n8n strict TypeScript | 100% (z√©ro `any`) | 100% |
| **Test coverage** | n8n 80%+ | 50% | 75% |
| **P95 scan latency** (6 signaux) | Dust 10M activities/day | < 5s | < 3s |
| **P95 agent execution** (draft simple) | Dust maxStepsPerRun:3 | < 10s | < 8s |
| **LLM cost per action** | n8n AI events | < ‚Ç¨0.08 | < ‚Ç¨0.05 |
| **Error rate** (scan) | Sentry | < 2% | < 0.5% |
| **Error rate** (agent) | Sentry | < 5% | < 1% |
| **Connector health** | Dust connector lifecycle | 95% | 99.5% |
| **Deployment time** (push ‚Üí prod) | n8n Docker CI | < 10min | < 5min |
| **Graceful shutdown** | n8n 30s timeout | 100% in-flight compl√©t√©s | 100% |
| **Credential encryption** | n8n | 100% Day 1 | 100% |

---

## 6. Go-To-Market

### 6.1 ICP

B2B services or tech SMEs, 10-500 employ√©s. Outils cloud (CRM, support, projets). Buyer initial : Sales Director ou Head of Ops ‚Äî le persona avec le plus de pain visible et qui partagera l'outil avec ses pairs. Expansion : Support, Marketing, HR via le seat model.

### 6.2 GTM : Pirate Mode + Communaut√© + Build in Public

**3 canaux, pas 1 :**

**Canal 1 ‚Äî Cold email (notre propre outbound engine)**
- 10-15 domaines, 25-30 mailboxes, ~‚Ç¨270-350/mois
- On utilise nos propres agents pour prospecter (dogfooding)
- 4-step s√©quences par persona √ó langue (FR + EN)
- M1-M3 : 5-7.5K emails/sem ‚Üí 50 users
- M4-M6 : scaled ‚Üí 200 users

**Canal 2 ‚Äî Communaut√© + contenu (le craft de l'AI business automation)**
- Communaut√© d√®s Day 1 (Slack ou Discord)
- Contenu hebdo : templates d'agents, r√©sultats r√©els, before/after
- "Agent of the Week" : un user showcas√©
- Build in public : Martin + Ombeline sur LinkedIn (FR + EN)
- Objectif Y2 : le contenu drive 70% de l'inbound (comme Lemlist Y2)

**Canal 3 ‚Äî R√©seau + referral**
- Les premiers users viennent du r√©seau perso + SF network
- Referral natif : "invitez un coll√®gue, 500 cr√©dits gratuits"
- Chaque user satisfait dans une PME = porte d'entr√©e vers toute l'entreprise

**Pas de Product Hunt Day 1. Pas d'AppSumo.** Le produit doit √™tre excellent avant l'exposition massive. Product Hunt = M3-M4 quand on a des case studies et des r√©sultats r√©els.

### 6.3 Pricing

| | Free | Starter | Pro | Business |
|--|------|---------|-----|----------|
| **Prix/mois** | ‚Ç¨0 | ‚Ç¨160 | ‚Ç¨350 | ‚Ç¨750 |
| **Cr√©dits** | 200 | 3,000 | 8,000 | 20,000 |
| **Si√®ges inclus** | 1 | 1 | 1 | 1 |
| **Si√®ge add.** | ‚Äî | +‚Ç¨15 | +‚Ç¨15 | +‚Ç¨15 |
| **Agents** | Illimit√©s | Illimit√©s | Illimit√©s | Illimit√©s |
| **Overage** | Hard stop | ‚Ç¨0.07/cr | ‚Ç¨0.06/cr | ‚Ç¨0.05/cr |

Transparence totale : dashboard temps r√©el, cr√©dits par agent, projection fin de mois.

**Le scan est GRATUIT.** M√™me en Free. C'est le hook. "3 deals dormants, 5 tickets proches SLA, 6 CV non trait√©s, ‚Ç¨12K factures overdue" ‚Üí l'user VEUT les agents pour corriger ‚Üí conversion.

**Co√ªt infra int√©grations (Pipedream Connect) : ~$2/user/mois.** Soit ~1.2% du revenue au Starter. Absorb√© dans la marge, invisible pour le client.

### 6.4 International

EN + FR Day 1. Tous march√©s anglophones + francophones en parall√®le.

---

## 7. Market Size & Path to ‚Ç¨100M ARR

Identique V4. Blended ACV Y1 : ‚Ç¨2,800. TAM Day 1 (sans USA) : ~135,000 SMEs = ‚Ç¨648M.

| Year | New accounts | Active | ARR | Growth |
|------|-------------|--------|-----|--------|
| Y2 | 200 | 200 | ‚Ç¨0.6M | ‚Äî |
| Y3 | 800 | 966 | ‚Ç¨2.9M | +422% |
| Y4 | 2,200 | 3,002 | ‚Ç¨9.7M | +233% |
| Y5 | 5,000 | 7,492 | ‚Ç¨25.9M | +166% |
| Y6 | 9,000 | 15,218 | ‚Ç¨56.8M | +119% |
| Y7 | 14,000 | 26,631 | **‚Ç¨108.4M** | +91% |

**Parall√®le Lemlist :** $252K ARR Y1 ‚Üí $720K Y2 ‚Üí $2M Y2.5 ‚Üí $6M Y3 ‚Üí $10M Y3.5 ‚Üí $20M Y5 ‚Üí $28M Y6 ‚Üí $40M Y7. Bootstrapped, 40% EBITDA, 50K clients.

---

## 8. Opportunity ‚Äî Why Now?

1. **Capability-reliability gap proven** ‚Äî METR, Klarna, IBM, DORA (2024-2025)
2. **SME AI adoption accelerating but shallow** ‚Äî 42% use AI, 5% automation (France Num 2025)
3. **LLM costs make contextual agents viable** ‚Äî contexte complet injectible pour <‚Ç¨0.01/draft
4. **EU AI Act** ‚Äî transparency requirements favor visible eval
5. **Gap between players** ‚Äî Dust = enterprise CTO. Lindy = SMB tech-aware qui sait ce qu'il veut automatiser. PME non-technique qui ne sait pas ce qu'elle rate = personne.

---

## 9. Competition

| Player | Forces | Ce qu'ils ne font PAS |
|--------|--------|-----------------------|
| **Statu quo (ChatGPT + manuellement)** | Gratuit, flexible, pas de setup | ‚ùå Pas de scan continu. ‚ùå Pas de connexion aux outils. ‚ùå Aucune automatisation. ‚ùå Le user doit tout faire lui-m√™me. |
| **Lindy** | 4,000+ int√©grations, 1,000+ templates, PMF prouv√© | ‚ùå Pas de scan ‚Ç¨ ("montre-moi ce que je perds"). ‚ùå Pas de style learner. ‚ùå L'user doit savoir ce qu'il veut automatiser. |
| **Dust** | Agent chaining, observability, SOC 2, Sequoia | ‚ùå Pas PME (enterprise only). ‚ùå Pas plug-and-play (n√©cessite admin IT). ‚ùå Pas de scan business. |
| **HubSpot AI** | Donn√©es CRM natives, base install√©e massive | ‚ùå Limit√© √† HubSpot. ‚ùå Pas d'agents autonomes cross-tools. ‚ùå Pas de scan multi-sources. |
| **Zapier** (indirect) | 7,000+ int√©grations, brand massive, simple | ‚ùå Trigger‚ÜíAction sans intelligence. Ne d√©tecte rien, n'√©crit rien, n'analyse rien. L'user doit tout configurer. |
| **Make** (indirect) | Sc√©narios visuels puissants, pricing agressif | ‚ùå M√™me limite que Zapier : automatisation r√©active, pas proactive. Ne g√©n√®re aucun contenu. |
| **n8n** (indirect) | Open-source, self-hosted, pas de lock-in | ‚ùå N√©cessite comp√©tences techniques. Cible devs, pas PME non-tech. |

**Le pari :** Le march√© des PME qui n'ont PAS encore d'agents est bien plus grand que le march√© des PME qui switchent de Lindy. On ne se bat pas contre Lindy ‚Äî on va chercher les gens qui ne connaissent pas Lindy. On ne se bat pas contre Zapier/Make ‚Äî ils automatisent de la plomberie, nous automatisons des m√©tiers. Le scan ‚Ç¨ les hook, les agents font le job, et le wedge se pr√©cise en usage.

**Ce qu'on vole √† chaque concurrent (d√©tail en section 2.1) :** Natural language agent creation + template quality (Lindy). Knowledge grounding + permissions granulaires (Dust). Template library searchable + documentation + fiabilit√© obsessionnelle (Zapier). Canvas visuel best-in-class + vue Grid (Make). Logs d√©taill√©s + debugging visible + code custom pour power users (n8n).

---

## 10. Hypotheses

| # | Hypothesis | Risk | Test |
|---|-----------|------|------|
| H1 | Le scan fram√© en ‚Ç¨ convertit >30% free ‚Üí agent activation | CRITICAL | M1-M3 |
| H2 | Les drafts contextuels sont significativement meilleurs que ChatGPT/concurrents | CRITICAL | Before/after A/B sur les premiers users |
| H3 | Le Style Learner r√©duit modification rate 70% ‚Üí <30% en 4 semaines | HIGH | Tracking par agent/user |
| H4 | Eval visible augmente la confiance vs agents aveugles | HIGH | NPS / feedback qualitatif |
| H5 | ~93 templates couvrent la majorit√© des besoins (pas besoin de builder Day 1) | HIGH | Feature requests hors catalogue, churn M3-M6 |
| H6 | Le dogfooding produit du contenu qui convertit | HIGH | Inbound attribution |
| H7 | La communaut√© sur le craft attire des prospects (pas juste des curieux) | MEDIUM | Conversion community ‚Üí signup |
| H8 | 2,800 APIs via Pipedream suffisent au launch | HIGH | "Int√©gration manquante" comme raison de churn |
| H9 | Dust ne descend pas PME en 18 mois | MEDIUM | Veille comp√©titive |
| H10 | Le vrai wedge √©merge en M1-M3 via les donn√©es d'usage (pas besoin de le fixer avant le launch) | HIGH | Tracking r√©tention par agent/persona/feature |

### Kill Criteria

- <20% compl√®tent le Business Scan ‚Üí Scan pas compelling
- Les drafts ne sont PAS per√ßus comme meilleurs que ChatGPT ‚Üí Thesis wrong
- >50% des users demandent des agents hors catalogue ‚Üí Acc√©l√©rer chat builder
- WTP <‚Ç¨120/mois ‚Üí Economics broken
- Style Learner ne r√©duit pas le taux de modification en M3 ‚Üí Feedback loop wrong
- Le dogfooding ne produit pas de contenu engageant ‚Üí Strat√©gie communaut√© √† revoir
- Aucun wedge n'√©merge en M3 (pas d'agent/feature/persona avec r√©tention sup√©rieure) ‚Üí Repositionner

---

## 11. La trajectoire produit (√† la Lemlist)

| Phase | Timeline | Produit | Ce qui d√©clenche le passage |
|-------|----------|---------|---------------------------|
| **V1** | M0-M3 | Plateforme agents AI pour PME non-tech. Scan ‚Ç¨ + ~93 templates (sales, marketing, support, HR, ops, research/product) + style learner + outbound infra. Tout via Pipedream. | Launch + 50 users |
| **V1.1** | M3-M5 | Impact Engine (attribution ‚Ç¨ aux agents). +templates guid√©s par feedback. Le wedge se pr√©cise. | Feedback, churn analysis, donn√©es d'usage |
| **V2** | M5-M9 | Chat builder simple. Visual agent editor. Marketplace embryonnaire. API. Doubler sur le wedge identifi√© en V1.1. | Users avanc√©s veulent du custom |
| **V3** | M9-M15 | Full platform. Marketplace. MCP pour extensions. | Scaler au-del√† des early adopters |
| **V4+** | M15+ | Enterprise tier, SSO, acquisitions compl√©mentaires | Revenue permet l'expansion |

**Le principe : chaque √©tape est d√©clench√©e par un signal march√©, pas par un plan.**

---

## 12. V5 ‚Üí V6 : Ce qui a chang√©

> **V6 = V5 inchang√© sur le produit, le GTM, le pricing, les hypoth√®ses. Seule la partie technique est consolid√©e √† partir du reverse-engineering des repos GitHub de Dust et n8n.**

| | V5 | V6 |
|---|---|---|
| **Orchestration agents** | Inngest (managed, vendor lock-in) | **BullMQ + Redis** (open-source, prouv√© par n8n √† 162K stars). Redis est d√©j√† n√©cessaire pour SSE + cache. |
| **LLM SDK** | Vercel AI SDK (abstraction multi-provider) | **@anthropic-ai/sdk direct** (Claude-only Day 1, pas d'overhead d'abstraction). Pattern Dust. |
| **Auth** | Better Auth | **Supabase Auth** (int√©gr√© √† notre Supabase PostgreSQL, OAuth social, magic link). |
| **Linting** | Biome | **ESLint + Prettier** (les DEUX repos utilisent ESLint, √©cosyst√®me plus riche). |
| **Monorepo** | Repo unique | **pnpm + Turborepo** (pattern n8n, builds parall√®les, caching <30s). |
| **State management** | Jotai | **Zustand** (plus simple pour React Flow). |
| **LLM tiers** | Gemini Flash / Haiku / Sonnet-GPT4o | **Haiku / Sonnet / Opus** (Claude-only, coh√©rent avec SDK direct). |
| **Streaming** | Non sp√©cifi√© | **SSE via Redis PubSub** (pattern Dust, pas de WebSocket). |
| **Credential security** | Non sp√©cifi√© | **AES-256 encryption + redaction** (pattern n8n, non-n√©gociable). |
| **Error handling** | Generic `Error` | **Error type hierarchy** (ScanError, AgentError, ConnectorError, CredentialError). |
| **Config management** | dotenv basique | **@Env() decorator + Zod validation** (pattern n8n, app refuse de d√©marrer si config invalide). |
| **Cost tracking** | Non sp√©cifi√© | **AI Event Logging** de chaque appel LLM (model, tokens, cost, latency, agent_id). |
| **Shutdown** | Non sp√©cifi√© | **Graceful shutdown 30s** (aucune ex√©cution coup√©e mid-run). |
| **Architecture patterns** | Non document√©s | **8 patterns prouv√©s** (Resource, Hooks, Encryption, Errors, Config, Events, SSE, Shutdown). |
| **Monorepo structure** | Non sp√©cifi√©e | **Architecture d√©taill√©e** (9 packages : types, db, core, connectors, queue, config, crypto, ai). |
| **D√©pendances** | Non list√©es | **Shopping list compl√®te** avec justification par source (Dust/n8n). |
| **M√©triques engineering** | Non d√©finies | **12 targets** (build time, type coverage, test coverage, latency, cost, error rate...). |

**Ce qui n'a PAS chang√© vs V5 :**
- Le produit : scan ‚Ç¨ + ~93 templates + style learner + eval L1/L2/L3
- Le dogfooding total comme pilier strat√©gique
- La communaut√© sur le craft
- Le GTM pirate mode + communaut√© + build in public
- Le pricing (Free/Starter/Pro/Business)
- Les hypoth√®ses et kill criteria
- La trajectoire produit Lemlist
- Le market size et path to ‚Ç¨100M ARR
- L'user journey Day 0 ‚Üí Month 2

---

*Document V6 ‚Äî Consolid√© le 9 f√©vrier 2026*
*Changement majeur vs V5 : consolidation technique bas√©e sur le reverse-engineering des repos GitHub de Dust.tt (84 contributeurs, 17,842 commits, $7.3M ARR) et n8n (162K stars, 400+ int√©grations). Stack revu de fond en comble : BullMQ au lieu d'Inngest, Anthropic SDK direct au lieu de Vercel AI SDK, pnpm+Turbo monorepo, 8 patterns architecturaux prouv√©s (Resource, Lifecycle Hooks, Credential Encryption, Error Hierarchy, Typed Config, AI Event Logging, SSE/Redis PubSub, Graceful Shutdown). Chaque choix est justifi√© par son usage en production dans l'un des deux repos. Le produit, le GTM, et la strat√©gie n'ont pas chang√©.*
